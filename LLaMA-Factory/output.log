[2024-12-05 08:51:47,005] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
WARNING 12-05 08:51:50 cuda.py:23] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.
12/05/2024 08:51:56 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26373
W1205 08:51:59.509000 402168 site-packages/torch/distributed/run.py:793] 
W1205 08:51:59.509000 402168 site-packages/torch/distributed/run.py:793] *****************************************
W1205 08:51:59.509000 402168 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1205 08:51:59.509000 402168 site-packages/torch/distributed/run.py:793] *****************************************
[2024-12-05 08:52:22,417] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-12-05 08:52:24,142] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 08:52:24,422] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 08:52:24,639] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-12-05 08:52:25,105] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 08:52:25,106] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 08:52:25,106] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 08:52:25,106] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH

[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-12-05 08:52:26,702] [INFO] [comm.py:637:init_distributed] cdb=None
12/05/2024 08:52:27 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[2024-12-05 08:52:28,312] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-12-05 08:52:28,399] [INFO] [comm.py:637:init_distributed] cdb=None
12/05/2024 08:52:28 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[rank1]:[W1205 08:52:28.148656390 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[2024-12-05 08:52:28,516] [INFO] [comm.py:637:init_distributed] cdb=None
12/05/2024 08:52:28 - INFO - llamafactory.hparams.parser - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
12/05/2024 08:52:28 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
12/05/2024 08:52:28 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[2024-12-05 08:52:29,329] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-12-05 08:52:29,329] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-12-05 08:52:29,329] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-12-05 08:52:29,329] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-12-05 08:52:29,330] [INFO] [comm.py:637:init_distributed] cdb=None
12/05/2024 08:52:29 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-12-05 08:52:29,562 >> loading configuration file /mnt/hwfile/mllm/weixilin/cache/Qwen2-VL-7B-Instruct-with-Qwen2-Language-Backbone/config.json
[INFO|configuration_utils.py:742] 2024-12-05 08:52:29,566 >> Model config Qwen2VLConfig {
  "_name_or_path": "/mnt/hwfile/mllm/weixilin/cache/Qwen2-VL-7B-Instruct-with-Qwen2-Language-Backbone",
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 66000,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "in_chans": 3,
    "model_type": "qwen2_vl",
    "spatial_patch_size": 14
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,569 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,569 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,569 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,569 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,569 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,569 >> loading file tokenizer_config.json
12/05/2024 08:52:29 - INFO - llamafactory.hparams.parser - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
12/05/2024 08:52:29 - INFO - llamafactory.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
12/05/2024 08:52:29 - INFO - llamafactory.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-12-05 08:52:29,985 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:373] 2024-12-05 08:52:29,986 >> loading configuration file /mnt/hwfile/mllm/weixilin/cache/Qwen2-VL-7B-Instruct-with-Qwen2-Language-Backbone/preprocessor_config.json
[INFO|image_processing_base.py:373] 2024-12-05 08:52:29,990 >> loading configuration file /mnt/hwfile/mllm/weixilin/cache/Qwen2-VL-7B-Instruct-with-Qwen2-Language-Backbone/preprocessor_config.json
[INFO|image_processing_base.py:429] 2024-12-05 08:52:29,990 >> Image processor Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "max_pixels": 12845056,
    "min_pixels": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,991 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,991 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,991 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,991 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,991 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-12-05 08:52:29,991 >> loading file tokenizer_config.json
12/05/2024 08:52:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[rank2]:[W1205 08:52:30.752608398 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
12/05/2024 08:52:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[rank7]:[W1205 08:52:30.760640644 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
12/05/2024 08:52:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[rank3]:[W1205 08:52:30.822553604 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[INFO|tokenization_utils_base.py:2470] 2024-12-05 08:52:30,387 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|processing_utils.py:744] 2024-12-05 08:52:30,867 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "max_pixels": 12845056,
    "min_pixels": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/mnt/hwfile/mllm/weixilin/cache/Qwen2-VL-7B-Instruct-with-Qwen2-Language-Backbone', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}

{
  "processor_class": "Qwen2VLProcessor"
}

12/05/2024 08:52:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
12/05/2024 08:52:30 - INFO - llamafactory.data.loader - Loading dataset llava-video-2_3mins-60k-remove_broken.json...
12/05/2024 08:52:31 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[rank6]:[W1205 08:52:31.719483673 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
12/05/2024 08:52:31 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[rank5]:[W1205 08:52:31.720775838 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
12/05/2024 08:52:31 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[rank4]:[W1205 08:52:31.723545337 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.

Converting format of dataset (num_proc=64):   0%|          | 0/59736 [00:00<?, ? examples/s]
Converting format of dataset (num_proc=64):   1%|          | 350/59736 [00:00<00:20, 2929.55 examples/s]
Converting format of dataset (num_proc=64):  61%|██████    | 36269/59736 [00:00<00:00, 196716.26 examples/s]
Converting format of dataset (num_proc=64):  96%|█████████▋| 57548/59736 [00:00<00:00, 154932.73 examples/s]
Converting format of dataset (num_proc=64): 100%|██████████| 59736/59736 [00:00<00:00, 102839.16 examples/s]
[rank0]:[W1205 08:53:52.386378058 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
12/05/2024 08:53:55 - INFO - llamafactory.data.loader - Loading dataset llava-video-2_3mins-60k-remove_broken.json...
12/05/2024 08:53:55 - INFO - llamafactory.data.loader - Loading dataset llava-video-2_3mins-60k-remove_broken.json...
12/05/2024 08:53:55 - INFO - llamafactory.data.loader - Loading dataset llava-video-2_3mins-60k-remove_broken.json...
12/05/2024 08:53:55 - INFO - llamafactory.data.loader - Loading dataset llava-video-2_3mins-60k-remove_broken.json...
12/05/2024 08:53:55 - INFO - llamafactory.data.loader - Loading dataset llava-video-2_3mins-60k-remove_broken.json...
12/05/2024 08:53:55 - INFO - llamafactory.data.loader - Loading dataset llava-video-2_3mins-60k-remove_broken.json...
12/05/2024 08:53:55 - INFO - llamafactory.data.loader - Loading dataset llava-video-2_3mins-60k-remove_broken.json...

Running tokenizer on dataset (num_proc=64):   0%|          | 0/59736 [00:00<?, ? examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 10/59736 [01:51<185:27:43, 11.18s/ examples]
Running tokenizer on dataset (num_proc=64):   0%|          | 20/59736 [01:58<83:04:59,  5.01s/ examples] 
Running tokenizer on dataset (num_proc=64):   0%|          | 30/59736 [01:58<45:15:05,  2.73s/ examples]
Running tokenizer on dataset (num_proc=64):   0%|          | 40/59736 [02:09<34:23:10,  2.07s/ examples]
Running tokenizer on dataset (num_proc=64):   0%|          | 50/59736 [02:11<23:10:45,  1.40s/ examples]
Running tokenizer on dataset (num_proc=64):   0%|          | 60/59736 [02:12<15:35:33,  1.06 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 70/59736 [02:13<11:09:36,  1.49 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 80/59736 [02:14<8:00:00,  2.07 examples/s] 
Running tokenizer on dataset (num_proc=64):   0%|          | 90/59736 [02:15<6:17:09,  2.64 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 100/59736 [02:16<4:37:13,  3.59 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 110/59736 [02:16<3:16:46,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 120/59736 [02:17<2:45:09,  6.02 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 130/59736 [02:17<2:12:57,  7.47 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 140/59736 [02:18<1:42:15,  9.71 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 150/59736 [02:20<2:16:34,  7.27 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 160/59736 [02:21<1:59:58,  8.28 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 170/59736 [02:21<1:27:08, 11.39 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 180/59736 [02:22<1:32:55, 10.68 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 190/59736 [02:22<1:11:06, 13.96 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 200/59736 [02:25<2:08:57,  7.69 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 210/59736 [02:27<2:31:13,  6.56 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 230/59736 [02:29<2:05:13,  7.92 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 240/59736 [02:30<2:20:01,  7.08 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 260/59736 [02:31<1:29:13, 11.11 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 270/59736 [02:33<1:57:33,  8.43 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 280/59736 [02:34<1:51:08,  8.92 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 290/59736 [02:35<1:49:56,  9.01 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 300/59736 [02:36<1:38:41, 10.04 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 310/59736 [02:36<1:33:53, 10.55 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 320/59736 [02:37<1:12:31, 13.66 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 330/59736 [02:38<1:26:09, 11.49 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 340/59736 [02:39<1:26:59, 11.38 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 350/59736 [02:41<2:08:13,  7.72 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 360/59736 [02:41<1:33:53, 10.54 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 370/59736 [02:42<1:19:33, 12.44 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 380/59736 [02:42<1:09:47, 14.17 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 390/59736 [02:43<1:26:36, 11.42 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 410/59736 [02:44<58:46, 16.82 examples/s]  
Running tokenizer on dataset (num_proc=64):   1%|          | 420/59736 [02:46<1:22:53, 11.93 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 430/59736 [02:46<1:11:30, 13.82 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 440/59736 [02:46<54:51, 18.02 examples/s]  
Running tokenizer on dataset (num_proc=64):   1%|          | 450/59736 [02:47<1:09:13, 14.27 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 460/59736 [02:49<1:54:45,  8.61 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 470/59736 [02:50<1:33:36, 10.55 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 480/59736 [02:50<1:24:23, 11.70 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 490/59736 [02:51<1:08:35, 14.39 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 500/59736 [02:51<54:18, 18.18 examples/s]  
Running tokenizer on dataset (num_proc=64):   1%|          | 510/59736 [02:51<45:56, 21.48 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 520/59736 [02:54<1:40:52,  9.78 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 530/59736 [02:55<1:54:22,  8.63 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 540/59736 [02:55<1:27:41, 11.25 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 550/59736 [02:58<2:17:52,  7.15 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 560/59736 [02:59<2:02:43,  8.04 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 570/59736 [03:06<5:01:01,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 580/59736 [03:07<4:01:19,  4.09 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 590/59736 [03:08<3:06:15,  5.29 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 600/59736 [03:10<3:08:30,  5.23 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 610/59736 [03:13<3:52:54,  4.23 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 620/59736 [03:20<5:56:16,  2.77 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 630/59736 [03:28<8:05:48,  2.03 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 640/59736 [03:28<5:59:41,  2.74 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 650/59736 [03:55<17:34:37,  1.07s/ examples]
Running tokenizer on dataset (num_proc=64):   1%|          | 660/59736 [03:57<13:03:54,  1.26 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 670/59736 [04:06<13:30:08,  1.22 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 680/59736 [04:11<11:57:21,  1.37 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 690/59736 [04:11<8:30:03,  1.93 examples/s] 
Running tokenizer on dataset (num_proc=64):   1%|          | 700/59736 [04:14<7:31:05,  2.18 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 710/59736 [04:19<7:16:28,  2.25 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 720/59736 [04:22<7:01:15,  2.33 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 730/59736 [04:26<6:45:20,  2.43 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|          | 740/59736 [04:30<6:28:30,  2.53 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 760/59736 [04:30<3:39:37,  4.48 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 770/59736 [04:34<4:15:56,  3.84 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 780/59736 [04:34<3:19:05,  4.94 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 790/59736 [04:36<3:15:40,  5.02 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 800/59736 [04:37<2:29:35,  6.57 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 810/59736 [04:37<2:14:01,  7.33 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 820/59736 [04:38<1:43:37,  9.48 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 830/59736 [04:40<2:06:55,  7.74 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 840/59736 [04:40<1:53:43,  8.63 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 850/59736 [04:42<1:53:19,  8.66 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 860/59736 [04:43<1:48:35,  9.04 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 870/59736 [04:45<2:22:30,  6.88 examples/s]
Running tokenizer on dataset (num_proc=64):   1%|▏         | 890/59736 [04:46<1:36:53, 10.12 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 900/59736 [04:47<1:44:46,  9.36 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 910/59736 [04:48<1:43:49,  9.44 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 920/59736 [04:50<1:54:28,  8.56 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 930/59736 [04:51<2:01:03,  8.10 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 940/59736 [04:51<1:30:23, 10.84 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 950/59736 [04:51<1:08:10, 14.37 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 960/59736 [04:53<1:24:21, 11.61 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 970/59736 [04:54<1:37:14, 10.07 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 980/59736 [04:59<3:45:02,  4.35 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 990/59736 [05:04<4:55:53,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1000/59736 [05:05<4:01:01,  4.06 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1020/59736 [05:06<2:32:39,  6.41 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1040/59736 [05:07<1:53:44,  8.60 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1050/59736 [05:12<3:16:26,  4.98 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1060/59736 [05:13<2:47:54,  5.82 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1070/59736 [05:20<5:06:16,  3.19 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1080/59736 [05:21<4:14:44,  3.84 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1090/59736 [05:22<3:12:30,  5.08 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1100/59736 [05:23<3:03:36,  5.32 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1110/59736 [05:24<2:30:44,  6.48 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1120/59736 [05:24<1:57:06,  8.34 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1130/59736 [05:25<1:38:30,  9.92 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1140/59736 [05:28<2:23:24,  6.81 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1150/59736 [05:28<2:04:51,  7.82 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1160/59736 [05:30<2:06:54,  7.69 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1180/59736 [05:35<2:59:31,  5.44 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1200/59736 [05:37<2:35:45,  6.26 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1210/59736 [05:37<2:06:13,  7.73 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1220/59736 [05:45<4:30:30,  3.61 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1230/59736 [05:45<3:24:30,  4.77 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1240/59736 [05:46<2:56:46,  5.52 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1250/59736 [05:52<4:45:52,  3.41 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1260/59736 [05:56<5:26:20,  2.99 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1270/59736 [05:57<4:09:20,  3.91 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1280/59736 [05:58<3:39:33,  4.44 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1290/59736 [06:02<4:25:16,  3.67 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1300/59736 [06:07<5:18:50,  3.05 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1320/59736 [06:18<7:02:14,  2.31 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1330/59736 [06:26<8:24:00,  1.93 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1340/59736 [06:29<7:32:26,  2.15 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1350/59736 [06:31<6:12:54,  2.61 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1360/59736 [06:34<6:03:51,  2.67 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1370/59736 [06:35<4:46:03,  3.40 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1380/59736 [06:37<4:19:33,  3.75 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1390/59736 [06:39<3:59:15,  4.06 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1400/59736 [06:41<3:41:50,  4.38 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1410/59736 [06:41<2:42:44,  5.97 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1420/59736 [06:41<2:03:31,  7.87 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1430/59736 [06:45<3:03:46,  5.29 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1450/59736 [06:46<2:12:27,  7.33 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1460/59736 [06:50<3:10:48,  5.09 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1470/59736 [06:52<3:13:30,  5.02 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1480/59736 [06:54<3:18:14,  4.90 examples/s]
Running tokenizer on dataset (num_proc=64):   2%|▏         | 1490/59736 [06:55<2:52:25,  5.63 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1500/59736 [07:00<4:00:28,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1510/59736 [07:01<3:25:21,  4.73 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1520/59736 [07:03<3:41:47,  4.37 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1530/59736 [07:05<3:30:10,  4.62 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1540/59736 [07:08<3:42:10,  4.37 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1550/59736 [07:09<2:55:29,  5.53 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1560/59736 [07:12<3:53:04,  4.16 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1570/59736 [07:13<2:54:25,  5.56 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1580/59736 [07:14<2:29:05,  6.50 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1590/59736 [07:15<2:20:29,  6.90 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1600/59736 [07:21<4:45:06,  3.40 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1610/59736 [07:26<5:34:39,  2.89 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1620/59736 [07:32<6:55:55,  2.33 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1630/59736 [07:35<6:05:17,  2.65 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1640/59736 [07:37<5:20:39,  3.02 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1650/59736 [07:38<4:03:41,  3.97 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1670/59736 [07:38<2:22:50,  6.78 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1680/59736 [07:40<2:36:10,  6.20 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1690/59736 [07:41<2:19:01,  6.96 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1700/59736 [07:44<2:48:45,  5.73 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1710/59736 [07:44<2:16:53,  7.06 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1720/59736 [07:48<3:24:22,  4.73 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1730/59736 [07:52<4:10:34,  3.86 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1750/59736 [07:59<4:54:18,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1760/59736 [08:00<4:03:05,  3.97 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1780/59736 [08:00<2:30:54,  6.40 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1790/59736 [08:01<2:10:41,  7.39 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1810/59736 [08:02<1:36:32, 10.00 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1820/59736 [08:07<3:04:44,  5.22 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1830/59736 [08:08<2:40:25,  6.02 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1840/59736 [08:09<2:30:22,  6.42 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1850/59736 [08:10<2:15:19,  7.13 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1860/59736 [08:14<3:05:31,  5.20 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1870/59736 [08:14<2:36:09,  6.18 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1880/59736 [08:16<2:23:34,  6.72 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1890/59736 [08:17<2:32:24,  6.33 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1900/59736 [08:19<2:40:22,  6.01 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1910/59736 [08:19<1:57:30,  8.20 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1920/59736 [08:21<2:04:46,  7.72 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1930/59736 [08:23<2:22:13,  6.77 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1940/59736 [08:24<2:22:36,  6.75 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1960/59736 [08:27<2:11:48,  7.31 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1970/59736 [08:28<2:14:18,  7.17 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1980/59736 [08:36<5:02:07,  3.19 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 1990/59736 [08:43<6:46:59,  2.36 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2000/59736 [08:47<6:26:59,  2.49 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2010/59736 [08:48<5:15:38,  3.05 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2020/59736 [08:50<4:30:48,  3.55 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2030/59736 [08:52<3:57:48,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2040/59736 [08:54<4:00:44,  3.99 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2050/59736 [08:55<2:56:43,  5.44 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2060/59736 [08:58<3:30:09,  4.57 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2070/59736 [09:00<3:39:34,  4.38 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2080/59736 [09:03<4:10:44,  3.83 examples/s]
Running tokenizer on dataset (num_proc=64):   3%|▎         | 2090/59736 [09:07<4:38:56,  3.44 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2100/59736 [09:07<3:23:50,  4.71 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2110/59736 [09:07<2:27:52,  6.50 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2120/59736 [09:10<3:05:02,  5.19 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2130/59736 [09:12<2:48:12,  5.71 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2140/59736 [09:15<3:44:48,  4.27 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2150/59736 [09:22<5:48:13,  2.76 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2170/59736 [09:25<4:20:05,  3.69 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2180/59736 [09:26<3:38:38,  4.39 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2190/59736 [09:27<3:09:59,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2200/59736 [09:28<2:22:44,  6.72 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2210/59736 [09:30<2:56:30,  5.43 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2220/59736 [09:31<2:27:36,  6.49 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2230/59736 [09:31<1:53:48,  8.42 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▎         | 2240/59736 [09:33<1:56:21,  8.24 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2250/59736 [09:37<3:24:54,  4.68 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2260/59736 [09:39<3:05:23,  5.17 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2270/59736 [09:39<2:29:19,  6.41 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2280/59736 [09:44<4:12:20,  3.79 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2290/59736 [09:49<5:01:14,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2300/59736 [09:49<3:47:15,  4.21 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2310/59736 [09:50<3:02:59,  5.23 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2320/59736 [09:56<4:56:38,  3.23 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2330/59736 [09:57<4:06:14,  3.89 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2340/59736 [10:00<4:08:30,  3.85 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2350/59736 [10:04<4:37:20,  3.45 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2360/59736 [10:10<6:12:19,  2.57 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2370/59736 [10:12<5:14:34,  3.04 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2380/59736 [10:13<4:29:47,  3.54 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2390/59736 [10:14<3:29:51,  4.55 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2400/59736 [10:29<9:28:15,  1.68 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2410/59736 [10:36<10:04:09,  1.58 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2420/59736 [10:38<7:48:13,  2.04 examples/s] 
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2430/59736 [10:39<6:16:20,  2.54 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2440/59736 [10:41<5:21:20,  2.97 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2450/59736 [10:42<3:58:21,  4.01 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2470/59736 [10:43<2:37:09,  6.07 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2480/59736 [10:44<2:08:44,  7.41 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2490/59736 [10:44<1:54:45,  8.31 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2500/59736 [10:45<1:47:59,  8.83 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2510/59736 [10:47<2:01:28,  7.85 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2520/59736 [10:51<3:11:32,  4.98 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2530/59736 [10:52<2:45:14,  5.77 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2540/59736 [10:54<2:50:26,  5.59 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2550/59736 [10:56<2:51:42,  5.55 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2560/59736 [10:56<2:16:34,  6.98 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2570/59736 [10:57<2:03:59,  7.68 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2580/59736 [11:01<3:12:24,  4.95 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2590/59736 [11:07<5:03:11,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2600/59736 [11:11<5:19:11,  2.98 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2610/59736 [11:11<3:59:02,  3.98 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2620/59736 [11:12<3:13:03,  4.93 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2630/59736 [11:15<3:38:18,  4.36 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2640/59736 [11:17<3:25:00,  4.64 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2650/59736 [11:21<4:38:32,  3.42 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2660/59736 [11:22<3:22:26,  4.70 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2670/59736 [11:22<2:29:22,  6.37 examples/s]
Running tokenizer on dataset (num_proc=64):   4%|▍         | 2680/59736 [11:24<2:42:27,  5.85 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2690/59736 [11:27<3:17:55,  4.80 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2700/59736 [11:29<3:08:53,  5.03 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2710/59736 [11:31<3:17:53,  4.80 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2720/59736 [11:33<3:24:08,  4.65 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2730/59736 [11:34<2:47:47,  5.66 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2740/59736 [11:36<2:42:22,  5.85 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2750/59736 [11:37<2:31:34,  6.27 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2760/59736 [11:40<2:54:38,  5.44 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2770/59736 [11:43<3:30:20,  4.51 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2780/59736 [11:50<5:46:03,  2.74 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2790/59736 [11:52<5:02:42,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2800/59736 [11:54<4:46:37,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2810/59736 [11:55<3:27:18,  4.58 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2820/59736 [12:00<4:50:49,  3.26 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2830/59736 [12:06<6:19:07,  2.50 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2840/59736 [12:07<4:45:25,  3.32 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2850/59736 [12:13<6:29:11,  2.44 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2860/59736 [12:14<4:41:38,  3.37 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2870/59736 [12:17<5:01:02,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2880/59736 [12:19<4:18:32,  3.67 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2890/59736 [12:23<4:44:28,  3.33 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2900/59736 [12:23<3:41:47,  4.27 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2910/59736 [12:24<2:44:01,  5.77 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2920/59736 [12:26<2:55:10,  5.41 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2930/59736 [12:28<3:17:35,  4.79 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2940/59736 [12:32<4:08:27,  3.81 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2950/59736 [12:36<4:30:41,  3.50 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2960/59736 [12:36<3:30:34,  4.49 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2970/59736 [12:42<5:07:26,  3.08 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▍         | 2980/59736 [12:55<9:53:55,  1.59 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 2990/59736 [12:56<6:59:10,  2.26 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3000/59736 [13:01<7:34:10,  2.08 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3010/59736 [13:02<5:26:45,  2.89 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3020/59736 [13:07<6:16:36,  2.51 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3030/59736 [13:11<6:31:00,  2.42 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3040/59736 [13:12<4:54:25,  3.21 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3050/59736 [13:13<3:42:47,  4.24 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3060/59736 [13:13<2:40:26,  5.89 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3070/59736 [13:16<3:14:36,  4.85 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3080/59736 [13:18<3:21:25,  4.69 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3090/59736 [13:19<2:46:57,  5.65 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3100/59736 [13:25<4:39:33,  3.38 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3110/59736 [13:25<3:20:01,  4.72 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3120/59736 [13:32<5:40:38,  2.77 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3140/59736 [13:33<3:31:44,  4.45 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3150/59736 [13:37<4:01:51,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3160/59736 [13:37<3:10:56,  4.94 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3170/59736 [13:42<4:23:33,  3.58 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3180/59736 [13:43<3:27:54,  4.53 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3190/59736 [13:44<3:08:40,  4.99 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3200/59736 [13:45<2:21:46,  6.65 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3210/59736 [13:45<1:54:37,  8.22 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3220/59736 [13:46<1:37:30,  9.66 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3230/59736 [13:46<1:14:37, 12.62 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3240/59736 [13:46<58:08, 16.19 examples/s]  
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3250/59736 [13:47<1:10:12, 13.41 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3260/59736 [13:50<2:02:16,  7.70 examples/s]
Running tokenizer on dataset (num_proc=64):   5%|▌         | 3280/59736 [13:55<3:11:27,  4.91 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3300/59736 [13:56<2:13:47,  7.03 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3310/59736 [14:02<3:40:52,  4.26 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3320/59736 [14:02<2:52:49,  5.44 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3330/59736 [14:05<3:08:09,  5.00 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3340/59736 [14:06<2:42:13,  5.79 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3350/59736 [14:06<2:00:45,  7.78 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3360/59736 [14:06<1:36:32,  9.73 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3370/59736 [14:14<4:40:57,  3.34 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3380/59736 [14:17<4:36:28,  3.40 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3390/59736 [14:21<5:19:39,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3410/59736 [14:23<3:25:09,  4.58 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3420/59736 [14:26<3:51:01,  4.06 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3430/59736 [14:35<6:13:45,  2.51 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3440/59736 [14:38<6:11:49,  2.52 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3450/59736 [14:39<4:49:29,  3.24 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3460/59736 [14:42<4:29:10,  3.48 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3470/59736 [14:45<4:44:40,  3.29 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3480/59736 [14:47<4:01:41,  3.88 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3490/59736 [14:48<3:38:56,  4.28 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3500/59736 [14:55<5:51:09,  2.67 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3510/59736 [15:02<7:15:31,  2.15 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3520/59736 [15:03<5:34:56,  2.80 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3530/59736 [15:08<6:03:55,  2.57 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3540/59736 [15:12<6:18:58,  2.47 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3550/59736 [15:15<5:41:35,  2.74 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3560/59736 [15:16<4:38:54,  3.36 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3570/59736 [15:17<3:30:23,  4.45 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3580/59736 [15:17<2:41:49,  5.78 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3590/59736 [15:22<3:54:38,  3.99 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3600/59736 [15:24<3:52:45,  4.02 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3610/59736 [15:30<5:34:47,  2.79 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3620/59736 [15:33<5:04:59,  3.07 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3630/59736 [15:38<5:52:04,  2.66 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3640/59736 [15:44<7:00:02,  2.23 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3650/59736 [15:47<6:22:39,  2.44 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3660/59736 [15:51<6:26:31,  2.42 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3670/59736 [15:55<6:00:50,  2.59 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3680/59736 [15:56<4:51:10,  3.21 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3690/59736 [15:59<5:00:42,  3.11 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3700/59736 [16:00<3:36:23,  4.32 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▌         | 3720/59736 [16:01<2:22:01,  6.57 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3740/59736 [16:02<1:56:06,  8.04 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3750/59736 [16:03<1:46:45,  8.74 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3760/59736 [16:04<1:30:27, 10.31 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3770/59736 [16:04<1:23:07, 11.22 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3780/59736 [16:08<2:36:18,  5.97 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3790/59736 [16:11<2:56:15,  5.29 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3800/59736 [16:23<7:32:39,  2.06 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3810/59736 [16:24<5:38:58,  2.75 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3820/59736 [16:27<5:39:56,  2.74 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3830/59736 [16:28<4:07:25,  3.77 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3840/59736 [16:31<4:24:44,  3.52 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3850/59736 [16:32<3:51:23,  4.03 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3860/59736 [16:35<3:52:24,  4.01 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3870/59736 [16:37<3:40:34,  4.22 examples/s]
Running tokenizer on dataset (num_proc=64):   6%|▋         | 3880/59736 [16:38<3:08:18,  4.94 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3890/59736 [16:42<4:05:17,  3.79 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3900/59736 [16:47<4:54:25,  3.16 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3910/59736 [16:48<4:02:11,  3.84 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3920/59736 [16:48<2:53:02,  5.38 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3930/59736 [16:48<2:04:27,  7.47 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3940/59736 [16:49<1:35:00,  9.79 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3950/59736 [16:49<1:16:19, 12.18 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3960/59736 [16:54<3:25:58,  4.51 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3970/59736 [16:59<4:22:38,  3.54 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3980/59736 [17:00<3:45:03,  4.13 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 3990/59736 [17:01<3:07:47,  4.95 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4000/59736 [17:02<2:28:04,  6.27 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4010/59736 [17:07<4:02:15,  3.83 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4020/59736 [17:08<3:15:11,  4.76 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4030/59736 [17:13<4:35:38,  3.37 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4040/59736 [17:15<4:21:13,  3.55 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4050/59736 [17:16<3:25:16,  4.52 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4060/59736 [17:18<3:23:56,  4.55 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4070/59736 [17:18<2:31:42,  6.12 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4080/59736 [17:20<2:32:01,  6.10 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4090/59736 [17:24<3:37:23,  4.27 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4100/59736 [17:30<5:08:55,  3.00 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4110/59736 [17:42<9:29:16,  1.63 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4120/59736 [17:47<8:53:09,  1.74 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4140/59736 [17:48<5:10:06,  2.99 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4150/59736 [17:49<4:09:38,  3.71 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4160/59736 [17:49<3:08:38,  4.91 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4170/59736 [17:50<2:36:21,  5.92 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4180/59736 [17:53<3:02:03,  5.09 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4190/59736 [17:56<3:33:51,  4.33 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4200/59736 [18:03<5:37:33,  2.74 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4210/59736 [18:07<5:40:52,  2.71 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4220/59736 [18:07<4:10:38,  3.69 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4230/59736 [18:12<5:21:18,  2.88 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4240/59736 [18:16<5:25:52,  2.84 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4250/59736 [18:17<4:28:03,  3.45 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4260/59736 [18:25<6:47:11,  2.27 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4270/59736 [18:26<4:53:52,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4280/59736 [18:26<3:32:22,  4.35 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4290/59736 [18:29<3:49:17,  4.03 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4300/59736 [18:29<2:43:41,  5.64 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4320/59736 [18:32<2:29:03,  6.20 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4330/59736 [18:35<3:00:06,  5.13 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4340/59736 [18:36<2:49:26,  5.45 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4350/59736 [18:38<2:50:03,  5.43 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4360/59736 [18:40<2:53:36,  5.32 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4370/59736 [18:41<2:30:33,  6.13 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4380/59736 [18:48<4:53:22,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4390/59736 [18:52<5:15:38,  2.92 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4400/59736 [18:54<4:45:01,  3.24 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4410/59736 [18:57<4:38:26,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4420/59736 [19:02<5:25:36,  2.83 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4430/59736 [19:03<4:30:46,  3.40 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4440/59736 [19:05<3:57:08,  3.89 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4450/59736 [19:11<5:20:00,  2.88 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4460/59736 [19:11<3:49:58,  4.01 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4470/59736 [19:16<4:54:24,  3.13 examples/s]
Running tokenizer on dataset (num_proc=64):   7%|▋         | 4480/59736 [19:18<4:26:40,  3.45 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4490/59736 [19:20<4:16:15,  3.59 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4500/59736 [19:21<3:07:06,  4.92 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4510/59736 [19:25<4:00:15,  3.83 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4520/59736 [19:26<3:37:02,  4.24 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4530/59736 [19:28<3:05:31,  4.96 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4540/59736 [19:28<2:21:15,  6.51 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4550/59736 [19:29<1:54:16,  8.05 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4560/59736 [19:29<1:27:22, 10.53 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4570/59736 [19:31<2:02:34,  7.50 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4580/59736 [19:32<1:49:33,  8.39 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4590/59736 [19:44<6:38:04,  2.31 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4600/59736 [19:46<5:50:53,  2.62 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4610/59736 [19:47<4:13:46,  3.62 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4620/59736 [19:53<5:51:53,  2.61 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4630/59736 [19:54<4:39:20,  3.29 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4640/59736 [19:58<4:52:34,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4650/59736 [19:58<3:47:06,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4660/59736 [20:01<3:48:25,  4.02 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4670/59736 [20:03<3:45:30,  4.07 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4680/59736 [20:04<2:56:15,  5.21 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4690/59736 [20:05<2:33:33,  5.97 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4700/59736 [20:17<7:05:57,  2.15 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4710/59736 [20:18<5:32:37,  2.76 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4720/59736 [20:27<7:51:51,  1.94 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4730/59736 [20:33<8:29:27,  1.80 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4740/59736 [20:35<6:56:24,  2.20 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4750/59736 [20:36<4:58:31,  3.07 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4760/59736 [20:36<3:46:13,  4.05 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4770/59736 [20:39<3:57:48,  3.85 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4780/59736 [20:43<4:27:07,  3.43 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4790/59736 [20:50<6:37:58,  2.30 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4800/59736 [20:51<4:44:05,  3.22 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4810/59736 [20:52<4:09:46,  3.67 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4820/59736 [20:53<3:11:45,  4.77 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4830/59736 [20:56<3:31:42,  4.32 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4840/59736 [20:57<3:11:09,  4.79 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4850/59736 [20:58<2:24:30,  6.33 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4860/59736 [21:03<4:10:33,  3.65 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4870/59736 [21:04<3:05:44,  4.92 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4880/59736 [21:05<2:35:49,  5.87 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4890/59736 [21:05<2:06:32,  7.22 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4900/59736 [21:08<2:32:41,  5.99 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4910/59736 [21:11<3:25:17,  4.45 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4920/59736 [21:15<4:11:34,  3.63 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4930/59736 [21:23<6:28:21,  2.35 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4940/59736 [21:23<4:44:46,  3.21 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4950/59736 [21:24<3:30:33,  4.34 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4960/59736 [21:24<2:36:28,  5.83 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4970/59736 [21:29<4:13:00,  3.61 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4980/59736 [21:34<4:59:33,  3.05 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 4990/59736 [21:38<5:30:29,  2.76 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 5000/59736 [21:41<5:06:16,  2.98 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 5010/59736 [21:48<6:32:54,  2.32 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 5020/59736 [21:52<6:38:54,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 5030/59736 [21:57<6:50:02,  2.22 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 5040/59736 [21:58<5:30:29,  2.76 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 5050/59736 [22:02<5:31:57,  2.75 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 5060/59736 [22:02<3:57:43,  3.83 examples/s]
Running tokenizer on dataset (num_proc=64):   8%|▊         | 5070/59736 [22:03<3:07:38,  4.86 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5080/59736 [22:06<3:22:46,  4.49 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5090/59736 [22:10<4:14:14,  3.58 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5100/59736 [22:11<3:42:39,  4.09 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5110/59736 [22:12<2:59:29,  5.07 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5120/59736 [22:12<2:10:17,  6.99 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5130/59736 [22:16<2:59:51,  5.06 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5140/59736 [22:18<2:58:26,  5.10 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5150/59736 [22:22<4:11:57,  3.61 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5160/59736 [22:24<3:49:35,  3.96 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5170/59736 [22:24<2:44:57,  5.51 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5180/59736 [22:28<3:31:02,  4.31 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5190/59736 [22:33<4:52:07,  3.11 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5200/59736 [22:33<3:28:38,  4.36 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5210/59736 [22:41<5:44:26,  2.64 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▊         | 5220/59736 [22:44<5:21:55,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5230/59736 [22:44<3:52:26,  3.91 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5240/59736 [22:47<3:54:52,  3.87 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5250/59736 [22:47<2:48:17,  5.40 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5260/59736 [22:47<2:14:42,  6.74 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5270/59736 [22:49<2:13:27,  6.80 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5280/59736 [22:49<1:47:43,  8.43 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5290/59736 [22:52<2:27:27,  6.15 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5300/59736 [22:53<2:14:33,  6.74 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5310/59736 [22:54<1:54:33,  7.92 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5320/59736 [22:54<1:38:11,  9.24 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5330/59736 [23:00<3:35:42,  4.20 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5340/59736 [23:03<3:44:00,  4.05 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5350/59736 [23:03<2:59:24,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5360/59736 [23:23<10:59:04,  1.38 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5370/59736 [23:25<8:31:09,  1.77 examples/s] 
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5380/59736 [23:25<6:11:40,  2.44 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5390/59736 [23:29<5:58:06,  2.53 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5400/59736 [23:30<4:42:38,  3.20 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5410/59736 [23:37<6:18:58,  2.39 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5420/59736 [23:38<4:47:45,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5430/59736 [23:40<4:17:56,  3.51 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5440/59736 [23:42<4:10:44,  3.61 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5450/59736 [23:50<6:12:24,  2.43 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5460/59736 [23:50<4:36:45,  3.27 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5470/59736 [23:52<4:13:49,  3.56 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5480/59736 [23:55<4:11:31,  3.60 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5490/59736 [23:58<4:26:53,  3.39 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5500/59736 [24:01<4:11:21,  3.60 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5510/59736 [24:04<4:30:56,  3.34 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5520/59736 [24:15<8:09:08,  1.85 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5530/59736 [24:20<7:55:06,  1.90 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5540/59736 [24:21<5:45:59,  2.61 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5550/59736 [24:23<5:08:18,  2.93 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5560/59736 [24:24<4:01:07,  3.74 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5570/59736 [24:25<3:20:19,  4.51 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5580/59736 [24:26<2:42:48,  5.54 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5600/59736 [24:26<1:34:39,  9.53 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5610/59736 [24:28<1:53:01,  7.98 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5630/59736 [24:34<2:49:43,  5.31 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5640/59736 [24:37<3:09:36,  4.75 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5650/59736 [24:37<2:38:44,  5.68 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5660/59736 [24:40<2:48:14,  5.36 examples/s]
Running tokenizer on dataset (num_proc=64):   9%|▉         | 5670/59736 [24:42<3:03:59,  4.90 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5680/59736 [24:43<2:39:57,  5.63 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5690/59736 [24:50<4:52:20,  3.08 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5700/59736 [24:52<4:06:14,  3.66 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5710/59736 [24:56<4:37:00,  3.25 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5720/59736 [24:57<3:42:46,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5730/59736 [24:59<3:50:08,  3.91 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5740/59736 [25:01<3:29:26,  4.30 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5750/59736 [25:02<2:44:30,  5.47 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5760/59736 [25:02<2:08:32,  7.00 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5770/59736 [25:05<2:38:20,  5.68 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5780/59736 [25:05<1:59:46,  7.51 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5790/59736 [25:09<3:04:40,  4.87 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5800/59736 [25:11<3:18:55,  4.52 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5810/59736 [25:20<6:01:01,  2.49 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5820/59736 [25:20<4:16:53,  3.50 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5830/59736 [25:21<3:19:14,  4.51 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5840/59736 [25:22<3:00:21,  4.98 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5850/59736 [25:23<2:35:36,  5.77 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5860/59736 [25:28<4:07:24,  3.63 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5870/59736 [25:30<3:32:24,  4.23 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5880/59736 [25:37<5:34:11,  2.69 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5900/59736 [25:37<3:09:14,  4.74 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5920/59736 [25:45<4:06:24,  3.64 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5930/59736 [25:45<3:27:37,  4.32 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5940/59736 [25:49<3:52:33,  3.86 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5950/59736 [25:54<5:02:02,  2.97 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5960/59736 [25:58<5:01:34,  2.97 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|▉         | 5970/59736 [25:58<3:46:07,  3.96 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 5980/59736 [26:00<3:24:02,  4.39 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 5990/59736 [26:01<3:01:51,  4.93 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6000/59736 [26:08<5:17:48,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6010/59736 [26:14<6:19:49,  2.36 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6020/59736 [26:17<5:46:10,  2.59 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6030/59736 [26:20<5:25:29,  2.75 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6040/59736 [26:21<4:15:39,  3.50 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6050/59736 [26:22<3:15:39,  4.57 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6060/59736 [26:27<4:17:14,  3.48 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6070/59736 [26:28<3:47:05,  3.94 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6080/59736 [26:38<6:54:02,  2.16 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6090/59736 [26:43<7:16:43,  2.05 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6100/59736 [26:48<7:01:56,  2.12 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6110/59736 [26:50<5:51:45,  2.54 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6120/59736 [26:51<4:42:37,  3.16 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6130/59736 [26:53<4:16:38,  3.48 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6140/59736 [26:55<3:50:00,  3.88 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6150/59736 [26:56<2:52:33,  5.18 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6160/59736 [26:56<2:21:01,  6.33 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6170/59736 [26:59<2:54:43,  5.11 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6180/59736 [27:00<2:30:41,  5.92 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6190/59736 [27:07<4:59:27,  2.98 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6200/59736 [27:08<3:54:18,  3.81 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6210/59736 [27:10<3:25:06,  4.35 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6230/59736 [27:10<2:01:09,  7.36 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6240/59736 [27:12<2:10:44,  6.82 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6250/59736 [27:19<4:08:50,  3.58 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6260/59736 [27:26<5:53:12,  2.52 examples/s]
Running tokenizer on dataset (num_proc=64):  10%|█         | 6270/59736 [27:27<4:37:49,  3.21 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6280/59736 [27:28<3:38:32,  4.08 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6290/59736 [27:28<2:51:31,  5.19 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6300/59736 [27:34<4:25:51,  3.35 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6310/59736 [27:36<3:53:14,  3.82 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6320/59736 [27:36<2:49:16,  5.26 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6330/59736 [27:36<2:10:06,  6.84 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6340/59736 [27:38<2:28:54,  5.98 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6350/59736 [27:40<2:27:32,  6.03 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6360/59736 [27:46<4:33:04,  3.26 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6370/59736 [27:47<3:24:58,  4.34 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6380/59736 [27:47<2:28:07,  6.00 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6390/59736 [27:48<2:14:54,  6.59 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6400/59736 [27:53<3:45:45,  3.94 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6410/59736 [27:59<5:15:11,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6420/59736 [28:07<7:12:12,  2.06 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6430/59736 [28:08<5:21:55,  2.76 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6440/59736 [28:10<4:50:09,  3.06 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6450/59736 [28:11<3:33:58,  4.15 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6460/59736 [28:16<4:47:34,  3.09 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6470/59736 [28:20<5:27:33,  2.71 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6480/59736 [28:21<4:01:36,  3.67 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6490/59736 [28:25<4:51:11,  3.05 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6500/59736 [28:28<4:18:13,  3.44 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6510/59736 [28:29<3:32:55,  4.17 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6520/59736 [28:30<2:55:41,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6530/59736 [28:33<3:39:54,  4.03 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6540/59736 [28:34<2:53:02,  5.12 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6550/59736 [28:37<3:22:07,  4.39 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6560/59736 [28:42<4:39:21,  3.17 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6570/59736 [28:47<5:30:37,  2.68 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6580/59736 [28:48<3:58:59,  3.71 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6590/59736 [28:51<4:17:50,  3.44 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6600/59736 [28:52<3:33:44,  4.14 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6610/59736 [28:55<3:43:03,  3.97 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6620/59736 [28:58<3:58:56,  3.71 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6630/59736 [29:02<4:23:03,  3.36 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6640/59736 [29:04<3:57:08,  3.73 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6650/59736 [29:06<3:33:26,  4.15 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6660/59736 [29:06<2:42:28,  5.44 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6670/59736 [29:16<6:07:41,  2.41 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6680/59736 [29:22<7:07:51,  2.07 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6690/59736 [29:24<5:38:58,  2.61 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6700/59736 [29:24<4:14:52,  3.47 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6710/59736 [29:25<3:28:42,  4.23 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█         | 6720/59736 [29:32<5:07:19,  2.88 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6730/59736 [29:33<4:09:15,  3.54 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6740/59736 [29:34<3:18:35,  4.45 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6750/59736 [29:37<3:40:01,  4.01 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6760/59736 [29:38<2:56:27,  5.00 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6770/59736 [29:38<2:23:45,  6.14 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6780/59736 [29:39<1:57:56,  7.48 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6790/59736 [29:43<3:03:33,  4.81 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6800/59736 [29:52<6:18:03,  2.33 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6810/59736 [29:54<4:59:19,  2.95 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6820/59736 [29:57<4:56:36,  2.97 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6830/59736 [29:57<3:31:35,  4.17 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6840/59736 [30:01<4:18:42,  3.41 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6850/59736 [30:02<3:23:07,  4.34 examples/s]
Running tokenizer on dataset (num_proc=64):  11%|█▏        | 6860/59736 [30:04<3:04:05,  4.79 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6870/59736 [30:04<2:14:24,  6.56 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6880/59736 [30:12<5:19:09,  2.76 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6890/59736 [30:15<4:59:54,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6900/59736 [30:17<4:20:29,  3.38 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6910/59736 [30:19<4:02:54,  3.62 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6920/59736 [30:20<3:13:04,  4.56 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6930/59736 [30:23<3:19:41,  4.41 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6940/59736 [30:28<4:41:29,  3.13 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6950/59736 [30:30<4:01:14,  3.65 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6960/59736 [30:30<3:01:42,  4.84 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6970/59736 [30:33<3:12:52,  4.56 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6980/59736 [30:34<2:50:07,  5.17 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 6990/59736 [30:35<2:23:48,  6.11 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7000/59736 [30:35<1:48:02,  8.13 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7010/59736 [30:37<1:46:31,  8.25 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7020/59736 [30:39<2:08:28,  6.84 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7030/59736 [30:43<3:37:29,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7040/59736 [30:49<4:46:56,  3.06 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7050/59736 [30:50<3:56:18,  3.72 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7060/59736 [30:51<3:08:16,  4.66 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7070/59736 [30:53<3:11:03,  4.59 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7080/59736 [30:55<3:07:16,  4.69 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7090/59736 [30:57<2:52:43,  5.08 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7100/59736 [31:01<3:53:25,  3.76 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7110/59736 [31:04<4:03:26,  3.60 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7120/59736 [31:08<4:28:25,  3.27 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7130/59736 [31:09<3:38:53,  4.01 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7140/59736 [31:11<3:22:25,  4.33 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7160/59736 [31:17<3:47:44,  3.85 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7170/59736 [31:25<5:42:24,  2.56 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7190/59736 [31:29<4:35:51,  3.17 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7200/59736 [31:39<7:01:22,  2.08 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7210/59736 [31:43<6:41:01,  2.18 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7230/59736 [31:49<5:43:37,  2.55 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7240/59736 [31:50<4:48:15,  3.04 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7250/59736 [31:50<3:43:26,  3.91 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7270/59736 [31:52<2:38:04,  5.53 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7280/59736 [31:53<2:26:13,  5.98 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7290/59736 [31:54<2:13:13,  6.56 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7300/59736 [31:56<2:22:38,  6.13 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7310/59736 [31:59<3:00:45,  4.83 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7320/59736 [32:01<2:49:22,  5.16 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7330/59736 [32:02<2:19:39,  6.25 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7340/59736 [32:05<2:53:05,  5.04 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7350/59736 [32:07<3:13:28,  4.51 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7360/59736 [32:15<5:35:10,  2.60 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7370/59736 [32:24<7:46:18,  1.87 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7380/59736 [32:30<8:10:53,  1.78 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7390/59736 [32:31<6:01:32,  2.41 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7400/59736 [32:34<5:23:53,  2.69 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7410/59736 [32:35<4:13:02,  3.45 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7420/59736 [32:35<3:01:09,  4.81 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7430/59736 [32:36<2:48:18,  5.18 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7440/59736 [32:40<3:33:30,  4.08 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7450/59736 [32:41<2:49:10,  5.15 examples/s]
Running tokenizer on dataset (num_proc=64):  12%|█▏        | 7460/59736 [32:43<2:49:32,  5.14 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7470/59736 [32:43<2:01:35,  7.16 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7480/59736 [32:47<3:16:19,  4.44 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7490/59736 [32:49<3:06:10,  4.68 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7500/59736 [32:56<5:08:19,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7510/59736 [32:58<4:39:32,  3.11 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7520/59736 [33:00<3:51:12,  3.76 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7530/59736 [33:04<4:36:47,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7540/59736 [33:10<5:46:00,  2.51 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7550/59736 [33:16<6:35:57,  2.20 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7560/59736 [33:20<6:20:09,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7570/59736 [33:20<4:29:05,  3.23 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7580/59736 [33:23<4:19:55,  3.34 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7590/59736 [33:23<3:13:57,  4.48 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7600/59736 [33:24<2:49:13,  5.13 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7610/59736 [33:28<3:25:03,  4.24 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7630/59736 [33:28<1:58:01,  7.36 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7640/59736 [33:29<1:54:31,  7.58 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7650/59736 [33:32<2:21:55,  6.12 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7660/59736 [33:36<3:24:59,  4.23 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7670/59736 [33:37<2:50:06,  5.10 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7680/59736 [33:38<2:29:59,  5.78 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7690/59736 [33:40<2:45:08,  5.25 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7700/59736 [33:44<3:38:17,  3.97 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7710/59736 [33:46<3:17:35,  4.39 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7720/59736 [33:52<4:40:38,  3.09 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7730/59736 [33:55<4:35:01,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7740/59736 [33:56<3:46:00,  3.83 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7750/59736 [34:10<8:36:18,  1.68 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7760/59736 [34:13<7:29:21,  1.93 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7770/59736 [34:19<7:43:43,  1.87 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7780/59736 [34:20<5:52:10,  2.46 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7790/59736 [34:21<4:40:29,  3.09 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7800/59736 [34:22<3:24:38,  4.23 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7810/59736 [34:24<3:27:05,  4.18 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7830/59736 [34:24<1:56:23,  7.43 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7840/59736 [34:27<2:23:47,  6.02 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7850/59736 [34:29<2:24:14,  6.00 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7860/59736 [34:35<4:03:44,  3.55 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7870/59736 [34:37<4:04:14,  3.54 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7880/59736 [34:39<3:28:55,  4.14 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7890/59736 [34:41<3:25:22,  4.21 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7900/59736 [34:42<3:01:05,  4.77 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7910/59736 [34:43<2:30:11,  5.75 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7920/59736 [34:44<1:58:11,  7.31 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7930/59736 [34:44<1:33:57,  9.19 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7940/59736 [34:54<5:09:42,  2.79 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7950/59736 [34:55<4:19:59,  3.32 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7960/59736 [34:59<4:35:16,  3.13 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7970/59736 [35:01<3:55:29,  3.66 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7980/59736 [35:09<6:15:58,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 7990/59736 [35:10<4:43:52,  3.04 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 8000/59736 [35:15<5:46:03,  2.49 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 8010/59736 [35:17<4:48:10,  2.99 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 8020/59736 [35:18<3:44:55,  3.83 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 8030/59736 [35:20<3:33:04,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 8040/59736 [35:21<2:46:14,  5.18 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 8050/59736 [35:24<3:29:06,  4.12 examples/s]
Running tokenizer on dataset (num_proc=64):  13%|█▎        | 8060/59736 [35:27<3:25:11,  4.20 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8070/59736 [35:28<2:59:30,  4.80 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8080/59736 [35:38<6:22:39,  2.25 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8090/59736 [35:39<4:53:54,  2.93 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8100/59736 [35:41<4:04:00,  3.53 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8110/59736 [35:49<6:23:35,  2.24 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8120/59736 [35:53<6:16:18,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8130/59736 [35:54<4:52:16,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8140/59736 [35:57<4:48:39,  2.98 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8150/59736 [35:59<3:52:51,  3.69 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8160/59736 [36:01<3:34:43,  4.00 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8170/59736 [36:11<7:07:58,  2.01 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8180/59736 [36:12<5:30:08,  2.60 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8190/59736 [36:13<3:57:07,  3.62 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8200/59736 [36:17<4:44:24,  3.02 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▎        | 8210/59736 [36:19<4:01:26,  3.56 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8220/59736 [36:20<3:11:44,  4.48 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8230/59736 [36:21<2:38:03,  5.43 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8240/59736 [36:21<2:08:50,  6.66 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8250/59736 [36:28<4:16:27,  3.35 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8260/59736 [36:31<4:09:55,  3.43 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8270/59736 [36:32<3:19:09,  4.31 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8280/59736 [36:34<3:28:32,  4.11 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8290/59736 [36:35<2:46:15,  5.16 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8300/59736 [36:36<2:20:02,  6.12 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8320/59736 [36:36<1:18:56, 10.85 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8330/59736 [36:38<1:37:22,  8.80 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8340/59736 [36:39<1:44:33,  8.19 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8350/59736 [36:42<2:11:41,  6.50 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8360/59736 [36:46<3:03:46,  4.66 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8370/59736 [36:47<2:43:41,  5.23 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8380/59736 [36:48<2:28:57,  5.75 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8390/59736 [36:51<2:59:39,  4.76 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8400/59736 [36:57<4:44:08,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8410/59736 [37:01<4:46:35,  2.98 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8420/59736 [37:05<4:56:53,  2.88 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8430/59736 [37:11<6:09:45,  2.31 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8440/59736 [37:12<4:56:16,  2.89 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8450/59736 [37:13<3:39:22,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8460/59736 [37:18<4:45:11,  3.00 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8470/59736 [37:18<3:23:21,  4.20 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8480/59736 [37:20<2:59:42,  4.75 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8490/59736 [37:23<3:24:26,  4.18 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8500/59736 [37:23<2:39:43,  5.35 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8510/59736 [37:33<6:08:19,  2.32 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8520/59736 [37:38<6:30:14,  2.19 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8530/59736 [37:43<6:32:16,  2.18 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8540/59736 [37:45<5:28:16,  2.60 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8550/59736 [37:46<4:06:31,  3.46 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8560/59736 [37:54<6:18:55,  2.25 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8570/59736 [37:54<4:36:20,  3.09 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8590/59736 [37:56<3:04:06,  4.63 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8600/59736 [37:59<3:11:58,  4.44 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8610/59736 [38:00<2:46:17,  5.12 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8620/59736 [38:05<4:03:19,  3.50 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8630/59736 [38:08<4:08:27,  3.43 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8640/59736 [38:09<3:26:29,  4.12 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8650/59736 [38:12<3:30:03,  4.05 examples/s]
Running tokenizer on dataset (num_proc=64):  14%|█▍        | 8660/59736 [38:17<4:42:41,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8670/59736 [38:18<3:36:04,  3.94 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8680/59736 [38:19<3:07:06,  4.55 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8690/59736 [38:23<3:38:09,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8700/59736 [38:25<3:35:43,  3.94 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8710/59736 [38:28<3:36:57,  3.92 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8720/59736 [38:29<2:48:34,  5.04 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8730/59736 [38:38<5:49:16,  2.43 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8740/59736 [38:38<4:25:39,  3.20 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8750/59736 [38:46<6:13:59,  2.27 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8760/59736 [38:46<4:27:17,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8770/59736 [38:53<5:55:37,  2.39 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8780/59736 [38:57<5:52:20,  2.41 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8790/59736 [39:01<5:52:42,  2.41 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8800/59736 [39:01<4:16:44,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8810/59736 [39:04<3:58:20,  3.56 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8830/59736 [39:06<3:02:18,  4.65 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8840/59736 [39:07<2:23:27,  5.91 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8850/59736 [39:10<3:01:51,  4.66 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8860/59736 [39:11<2:40:14,  5.29 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8870/59736 [39:18<4:27:18,  3.17 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8880/59736 [39:20<4:05:37,  3.45 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8890/59736 [39:23<3:59:54,  3.53 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8900/59736 [39:24<3:23:17,  4.17 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8910/59736 [39:24<2:34:00,  5.50 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8920/59736 [39:26<2:30:00,  5.65 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8930/59736 [39:28<2:24:13,  5.87 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8940/59736 [39:28<1:57:54,  7.18 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8950/59736 [39:34<3:42:32,  3.80 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▍        | 8960/59736 [39:34<2:48:53,  5.01 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 8970/59736 [39:39<3:48:58,  3.70 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 8980/59736 [39:45<5:14:21,  2.69 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 8990/59736 [39:46<4:20:24,  3.25 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9000/59736 [39:47<3:29:06,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9010/59736 [39:48<2:38:57,  5.32 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9030/59736 [39:49<1:38:56,  8.54 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9040/59736 [39:55<3:39:28,  3.85 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9050/59736 [40:01<4:40:40,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9060/59736 [40:04<4:32:39,  3.10 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9070/59736 [40:09<5:20:52,  2.63 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9080/59736 [40:10<4:15:07,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9090/59736 [40:11<3:19:57,  4.22 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9100/59736 [40:12<2:47:08,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9110/59736 [40:17<3:51:29,  3.64 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9120/59736 [40:24<5:59:30,  2.35 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9130/59736 [40:25<4:28:09,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9140/59736 [40:26<3:29:18,  4.03 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9150/59736 [40:28<3:30:10,  4.01 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9160/59736 [40:31<3:33:20,  3.95 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9170/59736 [40:32<2:42:44,  5.18 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9180/59736 [40:35<3:32:54,  3.96 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9190/59736 [40:40<4:33:13,  3.08 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9200/59736 [40:43<4:16:23,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9210/59736 [40:46<4:19:15,  3.25 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9220/59736 [40:51<5:15:27,  2.67 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9230/59736 [40:52<3:51:48,  3.63 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9240/59736 [40:54<3:30:15,  4.00 examples/s]
Running tokenizer on dataset (num_proc=64):  15%|█▌        | 9250/59736 [40:57<3:46:12,  3.72 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9260/59736 [40:57<2:46:44,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9270/59736 [40:59<2:43:45,  5.14 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9280/59736 [41:00<2:06:59,  6.62 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9290/59736 [41:00<1:36:03,  8.75 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9300/59736 [41:11<5:52:05,  2.39 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9310/59736 [41:15<5:46:24,  2.43 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9320/59736 [41:20<5:54:12,  2.37 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9330/59736 [41:26<6:54:04,  2.03 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9340/59736 [41:26<4:55:34,  2.84 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9350/59736 [41:27<3:39:45,  3.82 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9360/59736 [41:34<5:30:22,  2.54 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9370/59736 [41:36<4:45:51,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9380/59736 [41:43<6:23:52,  2.19 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9390/59736 [41:45<5:09:28,  2.71 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9410/59736 [41:46<2:56:10,  4.76 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9420/59736 [41:46<2:25:03,  5.78 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9430/59736 [41:49<2:51:45,  4.88 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9440/59736 [41:59<5:42:59,  2.44 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9450/59736 [42:00<4:33:45,  3.06 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9460/59736 [42:01<3:34:53,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9470/59736 [42:02<3:01:49,  4.61 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9480/59736 [42:02<2:12:33,  6.32 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9490/59736 [42:04<2:13:55,  6.25 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9500/59736 [42:08<3:35:46,  3.88 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9510/59736 [42:12<4:04:41,  3.42 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9520/59736 [42:17<4:59:25,  2.80 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9540/59736 [42:24<4:57:07,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9550/59736 [42:25<3:57:16,  3.53 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9560/59736 [42:26<3:17:15,  4.24 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9570/59736 [42:27<2:55:29,  4.76 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9580/59736 [42:32<3:47:28,  3.67 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9590/59736 [42:34<3:24:33,  4.09 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9600/59736 [42:36<3:34:57,  3.89 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9610/59736 [42:45<5:50:05,  2.39 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9620/59736 [42:46<4:48:20,  2.90 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9630/59736 [42:47<3:39:26,  3.81 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9640/59736 [42:48<2:59:16,  4.66 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9650/59736 [42:49<2:31:34,  5.51 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9660/59736 [42:51<2:41:10,  5.18 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9670/59736 [42:52<2:04:10,  6.72 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9680/59736 [42:52<1:30:18,  9.24 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9690/59736 [42:52<1:13:45, 11.31 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 9700/59736 [42:53<1:12:54, 11.44 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9710/59736 [42:57<2:19:57,  5.96 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9720/59736 [42:59<2:48:07,  4.96 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9730/59736 [43:01<2:33:41,  5.42 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9740/59736 [43:05<3:36:06,  3.86 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9750/59736 [43:07<3:24:31,  4.07 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9760/59736 [43:11<3:49:47,  3.62 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9770/59736 [43:17<5:24:43,  2.56 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9780/59736 [43:20<4:46:58,  2.90 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9790/59736 [43:23<4:35:19,  3.02 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9800/59736 [43:28<5:36:37,  2.47 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9810/59736 [43:32<5:16:47,  2.63 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9820/59736 [43:34<4:42:55,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9830/59736 [43:34<3:21:10,  4.13 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9840/59736 [43:39<4:21:50,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):  16%|█▋        | 9850/59736 [43:40<3:23:00,  4.10 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9860/59736 [43:41<2:38:54,  5.23 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9870/59736 [43:41<2:13:01,  6.25 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9880/59736 [43:43<2:21:06,  5.89 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9890/59736 [43:51<4:49:41,  2.87 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9900/59736 [44:00<7:13:01,  1.92 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9910/59736 [44:06<7:20:25,  1.89 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9920/59736 [44:09<6:31:43,  2.12 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9930/59736 [44:11<5:17:31,  2.61 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9940/59736 [44:14<4:49:50,  2.86 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9950/59736 [44:14<3:29:26,  3.96 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9960/59736 [44:14<2:37:31,  5.27 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9970/59736 [44:20<4:06:27,  3.37 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9980/59736 [44:22<3:59:44,  3.46 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 9990/59736 [44:23<3:12:34,  4.31 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10000/59736 [44:27<3:33:15,  3.89 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10010/59736 [44:28<3:11:43,  4.32 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10020/59736 [44:31<3:18:16,  4.18 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10030/59736 [44:32<2:55:32,  4.72 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10040/59736 [44:35<2:57:17,  4.67 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10050/59736 [44:36<2:29:01,  5.56 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10060/59736 [44:38<2:49:52,  4.87 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10070/59736 [44:39<2:07:18,  6.50 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10080/59736 [44:40<1:57:33,  7.04 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10090/59736 [44:44<2:58:31,  4.63 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10100/59736 [44:45<2:39:16,  5.19 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10110/59736 [44:49<3:32:48,  3.89 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10120/59736 [44:52<3:35:21,  3.84 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10130/59736 [44:56<4:18:58,  3.19 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10140/59736 [44:59<4:20:20,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10150/59736 [45:03<4:22:55,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10170/59736 [45:09<4:15:26,  3.23 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10180/59736 [45:12<4:26:57,  3.09 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10190/59736 [45:24<7:34:48,  1.82 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10200/59736 [45:24<5:37:55,  2.44 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10210/59736 [45:25<4:11:48,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10220/59736 [45:30<5:12:54,  2.64 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10230/59736 [45:31<3:57:39,  3.47 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10240/59736 [45:33<3:29:39,  3.93 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10250/59736 [45:36<3:40:42,  3.74 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10260/59736 [45:42<5:16:25,  2.61 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10270/59736 [45:45<4:39:02,  2.95 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10280/59736 [45:45<3:21:30,  4.09 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10290/59736 [45:45<2:33:12,  5.38 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10300/59736 [45:51<4:07:14,  3.33 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10310/59736 [45:52<3:17:30,  4.17 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10320/59736 [45:53<2:32:17,  5.41 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10340/59736 [45:58<3:02:06,  4.52 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10350/59736 [45:59<2:31:40,  5.43 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10360/59736 [46:04<3:53:49,  3.52 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10370/59736 [46:07<3:46:43,  3.63 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10380/59736 [46:12<4:43:40,  2.90 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10390/59736 [46:16<5:01:25,  2.73 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10400/59736 [46:17<3:56:50,  3.47 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10410/59736 [46:19<3:30:28,  3.91 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10420/59736 [46:21<3:09:16,  4.34 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10430/59736 [46:22<2:39:40,  5.15 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10440/59736 [46:25<3:07:39,  4.38 examples/s]
Running tokenizer on dataset (num_proc=64):  17%|█▋        | 10450/59736 [46:29<3:56:49,  3.47 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10460/59736 [46:31<3:41:01,  3.72 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10470/59736 [46:32<2:56:05,  4.66 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10480/59736 [46:35<3:06:54,  4.39 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10490/59736 [46:44<5:55:26,  2.31 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10500/59736 [46:46<4:53:13,  2.80 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10510/59736 [46:50<5:09:39,  2.65 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10520/59736 [46:52<4:12:42,  3.25 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10530/59736 [46:54<3:46:07,  3.63 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10540/59736 [46:58<4:32:35,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10550/59736 [47:01<4:09:47,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10560/59736 [47:01<3:17:09,  4.16 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10570/59736 [47:04<3:24:53,  4.00 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10580/59736 [47:09<4:31:07,  3.02 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10590/59736 [47:12<4:08:56,  3.29 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10600/59736 [47:12<2:56:43,  4.63 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10610/59736 [47:17<4:14:14,  3.22 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10620/59736 [47:20<4:14:15,  3.22 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10630/59736 [47:24<4:32:48,  3.00 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10640/59736 [47:26<4:03:00,  3.37 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10650/59736 [47:31<4:38:47,  2.93 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10660/59736 [47:34<4:23:16,  3.11 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10670/59736 [47:34<3:15:18,  4.19 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10680/59736 [47:36<2:54:46,  4.68 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10690/59736 [47:36<2:26:04,  5.60 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10700/59736 [47:37<2:05:01,  6.54 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10710/59736 [47:40<2:27:56,  5.52 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10720/59736 [47:42<2:29:35,  5.46 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10730/59736 [47:43<2:08:10,  6.37 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10740/59736 [47:44<1:51:39,  7.31 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10750/59736 [47:50<3:58:16,  3.43 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10760/59736 [47:50<2:53:18,  4.71 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10770/59736 [48:01<6:14:55,  2.18 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10780/59736 [48:06<6:30:46,  2.09 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10790/59736 [48:08<5:15:38,  2.58 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10800/59736 [48:09<3:59:59,  3.40 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10810/59736 [48:09<3:05:38,  4.39 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10820/59736 [48:10<2:35:59,  5.23 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10830/59736 [48:13<3:04:39,  4.41 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10840/59736 [48:15<2:44:37,  4.95 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10850/59736 [48:24<5:44:32,  2.36 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10860/59736 [48:25<4:30:56,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10870/59736 [48:26<3:20:11,  4.07 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10880/59736 [48:31<4:16:00,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10890/59736 [48:31<3:07:47,  4.34 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10900/59736 [48:32<2:25:28,  5.59 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10910/59736 [48:39<4:50:40,  2.80 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10920/59736 [48:41<4:11:30,  3.23 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10930/59736 [48:42<3:07:09,  4.35 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10940/59736 [48:44<3:01:41,  4.48 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10950/59736 [48:45<2:44:56,  4.93 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10960/59736 [48:46<2:06:44,  6.41 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10970/59736 [48:46<1:39:51,  8.14 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10980/59736 [48:47<1:34:38,  8.59 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 10990/59736 [48:49<1:37:29,  8.33 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 11000/59736 [48:57<4:41:19,  2.89 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 11010/59736 [49:01<4:39:41,  2.90 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 11020/59736 [49:03<4:01:55,  3.36 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 11030/59736 [49:06<4:05:48,  3.30 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 11040/59736 [49:08<3:47:43,  3.56 examples/s]
Running tokenizer on dataset (num_proc=64):  18%|█▊        | 11050/59736 [49:21<7:57:45,  1.70 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11060/59736 [49:24<6:56:28,  1.95 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11070/59736 [49:27<5:42:50,  2.37 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11080/59736 [49:32<6:08:47,  2.20 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11090/59736 [49:32<4:27:57,  3.03 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11100/59736 [49:33<3:20:16,  4.05 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11110/59736 [49:34<2:42:14,  5.00 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11120/59736 [49:39<3:52:00,  3.49 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11130/59736 [49:42<3:54:13,  3.46 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11140/59736 [49:44<3:38:21,  3.71 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11150/59736 [49:49<4:38:20,  2.91 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11160/59736 [49:50<3:40:29,  3.67 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11170/59736 [49:51<3:00:19,  4.49 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11180/59736 [49:52<2:29:07,  5.43 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11190/59736 [49:57<3:52:14,  3.48 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▊        | 11200/59736 [49:59<3:29:18,  3.86 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11210/59736 [50:00<2:55:24,  4.61 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11220/59736 [50:07<4:34:28,  2.95 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11230/59736 [50:10<4:30:13,  2.99 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11240/59736 [50:11<3:31:13,  3.83 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11250/59736 [50:12<2:45:07,  4.89 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11260/59736 [50:12<1:59:41,  6.75 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11270/59736 [50:13<2:01:36,  6.64 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11280/59736 [50:16<2:27:22,  5.48 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11290/59736 [50:16<1:53:44,  7.10 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11300/59736 [50:17<1:42:06,  7.91 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11310/59736 [50:19<1:58:39,  6.80 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11320/59736 [50:21<2:16:08,  5.93 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11330/59736 [50:23<2:17:02,  5.89 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11340/59736 [50:24<1:52:45,  7.15 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11350/59736 [50:24<1:27:53,  9.17 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11360/59736 [50:30<3:25:26,  3.92 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11370/59736 [50:37<5:00:53,  2.68 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11380/59736 [50:40<4:57:13,  2.71 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11390/59736 [50:53<8:47:48,  1.53 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11400/59736 [50:54<6:24:10,  2.10 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11410/59736 [50:57<5:48:57,  2.31 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11420/59736 [51:01<5:35:44,  2.40 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11430/59736 [51:03<4:45:39,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11440/59736 [51:03<3:26:08,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11450/59736 [51:04<2:30:38,  5.34 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11460/59736 [51:06<2:38:02,  5.09 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11470/59736 [51:10<3:20:06,  4.02 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11480/59736 [51:12<3:18:59,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11490/59736 [51:15<3:23:17,  3.96 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11500/59736 [51:16<2:48:54,  4.76 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11510/59736 [51:18<2:41:14,  4.98 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11520/59736 [51:19<2:27:34,  5.45 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11530/59736 [51:21<2:41:47,  4.97 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11540/59736 [51:23<2:39:24,  5.04 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11550/59736 [51:25<2:20:23,  5.72 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11560/59736 [51:26<2:10:37,  6.15 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11570/59736 [51:31<3:36:15,  3.71 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11580/59736 [51:33<3:13:32,  4.15 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11590/59736 [51:34<2:31:07,  5.31 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11600/59736 [51:35<2:16:36,  5.87 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11610/59736 [51:38<2:55:54,  4.56 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11620/59736 [51:49<6:23:39,  2.09 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11630/59736 [51:56<7:06:58,  1.88 examples/s]
Running tokenizer on dataset (num_proc=64):  19%|█▉        | 11640/59736 [51:57<5:41:09,  2.35 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11650/59736 [52:06<7:21:14,  1.82 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11660/59736 [52:14<8:28:34,  1.58 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11670/59736 [52:16<6:47:28,  1.97 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11690/59736 [52:17<3:58:41,  3.35 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11700/59736 [52:21<4:18:24,  3.10 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11710/59736 [52:25<4:20:21,  3.07 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11720/59736 [52:27<4:07:43,  3.23 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11730/59736 [52:28<3:13:07,  4.14 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11740/59736 [52:29<2:51:31,  4.66 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11760/59736 [52:30<1:39:46,  8.01 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11770/59736 [52:31<1:44:23,  7.66 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11780/59736 [52:32<1:27:10,  9.17 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11790/59736 [52:39<3:39:30,  3.64 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11800/59736 [52:42<3:55:21,  3.39 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11810/59736 [52:46<4:15:54,  3.12 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11820/59736 [52:54<6:08:56,  2.16 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11830/59736 [52:56<4:51:02,  2.74 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11840/59736 [52:56<3:28:57,  3.82 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11850/59736 [53:00<4:00:24,  3.32 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11860/59736 [53:01<3:10:47,  4.18 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11870/59736 [53:03<3:00:34,  4.42 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11880/59736 [53:03<2:25:28,  5.48 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11890/59736 [53:03<1:44:43,  7.61 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11900/59736 [53:04<1:17:03, 10.35 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11910/59736 [53:07<2:11:42,  6.05 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11920/59736 [53:10<2:53:49,  4.58 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11930/59736 [53:13<3:05:30,  4.30 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|█▉        | 11940/59736 [53:18<4:00:45,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 11950/59736 [53:24<5:17:26,  2.51 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 11960/59736 [53:25<4:03:52,  3.27 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 11970/59736 [53:28<4:05:23,  3.24 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 11980/59736 [53:32<4:40:50,  2.83 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 11990/59736 [53:33<3:33:12,  3.73 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12000/59736 [53:35<3:22:36,  3.93 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12010/59736 [53:37<2:53:11,  4.59 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12020/59736 [53:44<4:46:22,  2.78 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12030/59736 [53:44<3:35:25,  3.69 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12040/59736 [53:46<3:04:16,  4.31 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12050/59736 [53:51<4:17:07,  3.09 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12060/59736 [53:52<3:17:15,  4.03 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12070/59736 [53:52<2:21:42,  5.61 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12080/59736 [53:57<3:42:44,  3.57 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12090/59736 [53:59<3:17:50,  4.01 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12100/59736 [53:59<2:26:47,  5.41 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12110/59736 [54:05<4:06:24,  3.22 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12120/59736 [54:07<3:33:44,  3.71 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12130/59736 [54:10<3:43:08,  3.56 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12140/59736 [54:12<3:12:10,  4.13 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12150/59736 [54:13<2:50:43,  4.65 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12160/59736 [54:14<2:32:26,  5.20 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12170/59736 [54:15<1:59:55,  6.61 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12180/59736 [54:18<2:39:30,  4.97 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12190/59736 [54:20<2:42:05,  4.89 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12200/59736 [54:23<2:45:46,  4.78 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12210/59736 [54:25<2:44:26,  4.82 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12220/59736 [54:43<9:08:25,  1.44 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12230/59736 [54:44<7:01:47,  1.88 examples/s]
Running tokenizer on dataset (num_proc=64):  20%|██        | 12240/59736 [54:46<5:41:26,  2.32 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12250/59736 [54:48<4:32:16,  2.91 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12260/59736 [54:53<5:24:00,  2.44 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12270/59736 [54:57<5:06:32,  2.58 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12280/59736 [54:57<3:52:05,  3.41 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12290/59736 [54:59<3:07:32,  4.22 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12300/59736 [55:08<5:43:48,  2.30 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12310/59736 [55:11<5:22:51,  2.45 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12320/59736 [55:12<4:04:14,  3.24 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12330/59736 [55:14<3:36:32,  3.65 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12340/59736 [55:18<4:08:12,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12350/59736 [55:22<4:39:37,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12360/59736 [55:24<3:45:58,  3.49 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12370/59736 [55:25<3:03:19,  4.31 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12380/59736 [55:25<2:22:22,  5.54 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12400/59736 [55:28<2:14:47,  5.85 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12410/59736 [55:31<2:42:35,  4.85 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12420/59736 [55:32<2:08:30,  6.14 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12430/59736 [55:40<4:17:07,  3.07 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12440/59736 [55:41<3:42:28,  3.54 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12450/59736 [55:42<3:00:36,  4.36 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12460/59736 [55:47<3:59:27,  3.29 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12470/59736 [55:54<5:23:40,  2.43 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12480/59736 [55:58<5:15:40,  2.49 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12490/59736 [55:58<4:00:15,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12500/59736 [56:00<3:26:24,  3.81 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12510/59736 [56:02<3:09:21,  4.16 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12520/59736 [56:03<2:38:07,  4.98 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12530/59736 [56:12<5:21:09,  2.45 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12550/59736 [56:13<3:07:31,  4.19 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12560/59736 [56:15<3:12:12,  4.09 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12570/59736 [56:18<3:16:02,  4.01 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12580/59736 [56:19<2:41:34,  4.86 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12590/59736 [56:21<2:39:27,  4.93 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12600/59736 [56:25<3:22:45,  3.87 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12610/59736 [56:25<2:41:17,  4.87 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12620/59736 [56:27<2:24:12,  5.45 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12630/59736 [56:33<4:00:28,  3.26 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12650/59736 [56:34<2:40:57,  4.88 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12660/59736 [56:45<5:34:35,  2.34 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12670/59736 [56:48<4:54:07,  2.67 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12680/59736 [56:51<4:53:11,  2.67 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██        | 12690/59736 [56:52<3:36:39,  3.62 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12700/59736 [56:53<2:56:58,  4.43 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12710/59736 [56:54<2:39:33,  4.91 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12720/59736 [56:54<1:55:35,  6.78 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12740/59736 [56:55<1:10:56, 11.04 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12750/59736 [56:57<1:32:45,  8.44 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12760/59736 [57:02<2:43:59,  4.77 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12770/59736 [57:02<2:08:15,  6.10 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12780/59736 [57:13<5:27:36,  2.39 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12790/59736 [57:14<4:18:58,  3.02 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12800/59736 [57:17<4:07:08,  3.17 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12810/59736 [57:19<3:54:56,  3.33 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12820/59736 [57:28<6:06:53,  2.13 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12830/59736 [57:31<5:28:43,  2.38 examples/s]
Running tokenizer on dataset (num_proc=64):  21%|██▏       | 12840/59736 [57:32<4:07:58,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12850/59736 [57:33<3:13:06,  4.05 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12860/59736 [57:36<3:46:18,  3.45 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12870/59736 [57:38<3:08:07,  4.15 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12880/59736 [57:40<3:08:12,  4.15 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12890/59736 [57:45<4:13:44,  3.08 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12900/59736 [57:52<5:40:22,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12910/59736 [57:54<4:43:33,  2.75 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12920/59736 [57:59<5:08:51,  2.53 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12930/59736 [58:04<5:31:15,  2.36 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12940/59736 [58:04<3:55:37,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12950/59736 [58:06<3:28:46,  3.74 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12960/59736 [58:07<2:47:46,  4.65 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12970/59736 [58:11<3:42:17,  3.51 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12980/59736 [58:17<4:53:48,  2.65 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 12990/59736 [58:19<4:00:10,  3.24 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13000/59736 [58:21<3:50:14,  3.38 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13010/59736 [58:22<2:56:43,  4.41 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13020/59736 [58:24<2:43:33,  4.76 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13030/59736 [58:28<3:23:54,  3.82 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13040/59736 [58:29<2:53:40,  4.48 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13050/59736 [58:29<2:14:17,  5.79 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13060/59736 [58:30<1:41:09,  7.69 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13070/59736 [58:31<1:36:07,  8.09 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13080/59736 [58:36<3:14:47,  3.99 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13090/59736 [58:37<2:40:24,  4.85 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13100/59736 [58:40<3:02:03,  4.27 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13110/59736 [58:41<2:20:04,  5.55 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13120/59736 [58:47<4:12:06,  3.08 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13140/59736 [58:49<2:38:09,  4.91 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13150/59736 [58:53<3:28:18,  3.73 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13160/59736 [59:05<6:26:52,  2.01 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13170/59736 [59:06<4:58:00,  2.60 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13180/59736 [59:07<3:55:40,  3.29 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13190/59736 [59:08<3:10:51,  4.06 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13200/59736 [59:08<2:33:33,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13210/59736 [59:09<1:51:59,  6.92 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13220/59736 [59:24<7:00:53,  1.84 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13230/59736 [59:31<7:57:14,  1.62 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13240/59736 [59:32<5:41:40,  2.27 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13250/59736 [59:33<4:38:37,  2.78 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13260/59736 [59:38<5:04:21,  2.54 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13270/59736 [59:44<5:41:54,  2.27 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13280/59736 [59:52<7:14:46,  1.78 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13290/59736 [59:53<5:26:33,  2.37 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13300/59736 [59:53<3:53:17,  3.32 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13310/59736 [59:54<3:00:03,  4.30 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13320/59736 [59:58<3:38:57,  3.53 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13340/59736 [59:59<2:20:39,  5.50 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13350/59736 [1:00:02<2:38:39,  4.87 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13360/59736 [1:00:03<2:13:52,  5.77 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13380/59736 [1:00:04<1:43:59,  7.43 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13390/59736 [1:00:05<1:21:42,  9.45 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13400/59736 [1:00:07<1:46:16,  7.27 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13410/59736 [1:00:18<4:54:04,  2.63 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13430/59736 [1:00:20<3:22:57,  3.80 examples/s]
Running tokenizer on dataset (num_proc=64):  22%|██▏       | 13440/59736 [1:00:21<2:52:42,  4.47 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13450/59736 [1:00:21<2:20:34,  5.49 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13460/59736 [1:00:27<3:31:37,  3.64 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13470/59736 [1:00:32<4:25:46,  2.90 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13480/59736 [1:00:34<3:55:53,  3.27 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13490/59736 [1:00:35<3:02:27,  4.22 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13500/59736 [1:00:38<3:16:19,  3.93 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13510/59736 [1:00:38<2:31:13,  5.09 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13520/59736 [1:00:40<2:29:37,  5.15 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13530/59736 [1:00:41<2:02:05,  6.31 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13540/59736 [1:00:42<2:02:33,  6.28 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13550/59736 [1:00:43<1:35:24,  8.07 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13560/59736 [1:00:46<2:25:48,  5.28 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13570/59736 [1:00:57<5:55:50,  2.16 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13580/59736 [1:00:59<4:56:45,  2.59 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13590/59736 [1:01:00<3:48:19,  3.37 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13600/59736 [1:01:10<6:25:45,  1.99 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13610/59736 [1:01:11<5:01:45,  2.55 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13620/59736 [1:01:13<4:20:11,  2.95 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13640/59736 [1:01:21<4:29:38,  2.85 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13650/59736 [1:01:22<3:40:37,  3.48 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13660/59736 [1:01:25<3:54:02,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13670/59736 [1:01:29<4:02:57,  3.16 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13680/59736 [1:01:31<3:39:24,  3.50 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13700/59736 [1:01:32<2:22:31,  5.38 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13710/59736 [1:01:34<2:31:48,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13720/59736 [1:01:37<2:41:11,  4.76 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13730/59736 [1:01:38<2:12:45,  5.78 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13740/59736 [1:01:41<3:00:13,  4.25 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13750/59736 [1:01:45<3:19:33,  3.84 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13760/59736 [1:01:54<5:48:24,  2.20 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13770/59736 [1:01:58<5:35:10,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13780/59736 [1:01:59<4:19:40,  2.95 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13800/59736 [1:02:00<2:29:26,  5.12 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13810/59736 [1:02:00<2:05:54,  6.08 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13820/59736 [1:02:01<1:49:35,  6.98 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13830/59736 [1:02:06<3:09:09,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13840/59736 [1:02:15<5:14:27,  2.43 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13850/59736 [1:02:16<4:23:54,  2.90 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13860/59736 [1:02:28<7:17:57,  1.75 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13870/59736 [1:02:30<5:48:42,  2.19 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13880/59736 [1:02:30<4:15:10,  3.00 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13890/59736 [1:02:37<5:32:57,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13910/59736 [1:02:41<4:13:52,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13920/59736 [1:02:42<3:24:04,  3.74 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13930/59736 [1:02:48<4:30:21,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13940/59736 [1:02:50<4:04:34,  3.12 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13950/59736 [1:02:51<3:16:45,  3.88 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13960/59736 [1:02:56<4:13:21,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13970/59736 [1:02:57<3:26:07,  3.70 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13980/59736 [1:02:59<3:11:40,  3.98 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 13990/59736 [1:03:00<2:33:40,  4.96 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 14000/59736 [1:03:05<3:33:24,  3.57 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 14010/59736 [1:03:07<3:20:48,  3.80 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 14020/59736 [1:03:08<2:35:48,  4.89 examples/s]
Running tokenizer on dataset (num_proc=64):  23%|██▎       | 14030/59736 [1:03:09<2:16:24,  5.58 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14040/59736 [1:03:15<3:51:00,  3.30 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14050/59736 [1:03:16<3:06:04,  4.09 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14060/59736 [1:03:21<4:01:27,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14070/59736 [1:03:22<3:18:39,  3.83 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14080/59736 [1:03:24<3:12:43,  3.95 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14090/59736 [1:03:25<2:25:30,  5.23 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14100/59736 [1:03:29<3:12:31,  3.95 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14110/59736 [1:03:31<3:01:10,  4.20 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14120/59736 [1:03:31<2:11:11,  5.79 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14130/59736 [1:03:31<1:39:38,  7.63 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14140/59736 [1:03:33<1:34:40,  8.03 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14150/59736 [1:03:34<1:36:59,  7.83 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14160/59736 [1:03:41<3:43:24,  3.40 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14170/59736 [1:03:41<2:47:11,  4.54 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▎       | 14180/59736 [1:03:42<2:07:29,  5.96 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14190/59736 [1:03:42<1:34:28,  8.03 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14200/59736 [1:03:44<1:52:13,  6.76 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14210/59736 [1:03:47<2:30:14,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14220/59736 [1:03:51<3:23:55,  3.72 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14230/59736 [1:03:56<4:10:04,  3.03 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14240/59736 [1:03:56<3:00:38,  4.20 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14250/59736 [1:04:06<5:39:41,  2.23 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14260/59736 [1:04:08<4:45:12,  2.66 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14270/59736 [1:04:09<3:38:45,  3.46 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14280/59736 [1:04:09<2:49:49,  4.46 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14290/59736 [1:04:16<4:29:47,  2.81 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14300/59736 [1:04:23<5:49:55,  2.16 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14310/59736 [1:04:24<4:32:18,  2.78 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14320/59736 [1:04:29<5:03:35,  2.49 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14330/59736 [1:04:29<3:34:51,  3.52 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14340/59736 [1:04:32<3:25:52,  3.67 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14350/59736 [1:04:34<3:14:02,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14360/59736 [1:04:35<2:37:20,  4.81 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14370/59736 [1:04:42<4:25:10,  2.85 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14380/59736 [1:04:44<3:46:13,  3.34 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14390/59736 [1:04:49<4:28:22,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14400/59736 [1:04:50<3:33:26,  3.54 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14410/59736 [1:04:55<4:30:31,  2.79 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14420/59736 [1:04:57<4:05:13,  3.08 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14430/59736 [1:04:59<3:24:24,  3.69 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14440/59736 [1:05:01<3:21:01,  3.76 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14450/59736 [1:05:02<2:25:25,  5.19 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14460/59736 [1:05:04<2:24:19,  5.23 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14480/59736 [1:05:09<2:56:17,  4.28 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14490/59736 [1:05:16<4:21:40,  2.88 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14500/59736 [1:05:17<3:32:07,  3.55 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14510/59736 [1:05:20<3:24:15,  3.69 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14520/59736 [1:05:22<3:13:25,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14530/59736 [1:05:24<3:15:59,  3.84 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14540/59736 [1:05:33<5:22:53,  2.33 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14550/59736 [1:05:35<4:35:04,  2.74 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14560/59736 [1:05:35<3:24:35,  3.68 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14570/59736 [1:05:38<3:29:17,  3.60 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14580/59736 [1:05:40<2:56:20,  4.27 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14590/59736 [1:05:40<2:11:51,  5.71 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14600/59736 [1:05:40<1:42:06,  7.37 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14610/59736 [1:05:42<1:53:30,  6.63 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14620/59736 [1:05:48<3:17:38,  3.80 examples/s]
Running tokenizer on dataset (num_proc=64):  24%|██▍       | 14630/59736 [1:05:48<2:21:11,  5.32 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14640/59736 [1:05:49<2:12:09,  5.69 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14650/59736 [1:05:51<2:14:49,  5.57 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14660/59736 [1:05:53<2:09:37,  5.80 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14670/59736 [1:05:54<2:10:47,  5.74 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14680/59736 [1:05:55<1:37:01,  7.74 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14690/59736 [1:05:55<1:25:46,  8.75 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14700/59736 [1:05:57<1:43:27,  7.25 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14710/59736 [1:06:00<2:08:58,  5.82 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14720/59736 [1:06:01<2:03:35,  6.07 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14730/59736 [1:06:02<1:33:32,  8.02 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14740/59736 [1:06:04<1:52:27,  6.67 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14750/59736 [1:06:06<2:03:01,  6.09 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14760/59736 [1:06:08<2:16:22,  5.50 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14770/59736 [1:06:09<1:51:59,  6.69 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14780/59736 [1:06:11<2:00:48,  6.20 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14790/59736 [1:06:16<3:25:16,  3.65 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14800/59736 [1:06:17<2:45:07,  4.54 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14810/59736 [1:06:18<2:12:00,  5.67 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14820/59736 [1:06:25<4:22:34,  2.85 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14830/59736 [1:06:27<3:33:19,  3.51 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14840/59736 [1:06:33<5:05:32,  2.45 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14850/59736 [1:06:34<3:51:46,  3.23 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14860/59736 [1:06:36<3:14:29,  3.85 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14870/59736 [1:06:37<2:53:11,  4.32 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14880/59736 [1:06:38<2:20:40,  5.31 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14890/59736 [1:06:39<1:58:30,  6.31 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14900/59736 [1:06:40<1:40:17,  7.45 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14910/59736 [1:06:41<1:44:24,  7.16 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14920/59736 [1:06:46<2:48:27,  4.43 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▍       | 14930/59736 [1:06:47<2:17:08,  5.45 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 14940/59736 [1:06:50<2:45:41,  4.51 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 14950/59736 [1:06:51<2:27:31,  5.06 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 14960/59736 [1:06:54<2:49:42,  4.40 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 14970/59736 [1:06:56<2:32:54,  4.88 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 14980/59736 [1:06:58<2:36:24,  4.77 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 14990/59736 [1:07:02<3:32:59,  3.50 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15000/59736 [1:07:07<4:18:48,  2.88 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15010/59736 [1:07:08<3:15:17,  3.82 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15020/59736 [1:07:14<4:26:48,  2.79 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15030/59736 [1:07:15<3:43:14,  3.34 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15040/59736 [1:07:20<4:13:03,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15050/59736 [1:07:23<4:08:49,  2.99 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15060/59736 [1:07:35<7:25:15,  1.67 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15070/59736 [1:07:45<8:45:18,  1.42 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15080/59736 [1:07:48<7:20:34,  1.69 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15090/59736 [1:07:49<5:30:58,  2.25 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15100/59736 [1:07:51<4:35:18,  2.70 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15110/59736 [1:07:53<3:57:05,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15120/59736 [1:07:57<4:11:58,  2.95 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15130/59736 [1:08:02<4:53:45,  2.53 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15140/59736 [1:08:04<4:08:55,  2.99 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15150/59736 [1:08:05<3:08:08,  3.95 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15160/59736 [1:08:06<2:49:43,  4.38 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15170/59736 [1:08:08<2:36:56,  4.73 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15180/59736 [1:08:18<5:24:56,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15190/59736 [1:08:19<4:06:51,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15200/59736 [1:08:22<4:16:35,  2.89 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15210/59736 [1:08:24<3:33:38,  3.47 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15220/59736 [1:08:26<3:10:23,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):  25%|██▌       | 15230/59736 [1:08:26<2:30:43,  4.92 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15240/59736 [1:08:27<1:50:55,  6.69 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15250/59736 [1:08:27<1:33:14,  7.95 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15260/59736 [1:08:28<1:12:02, 10.29 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15270/59736 [1:08:28<57:58, 12.78 examples/s]  
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15280/59736 [1:08:29<1:06:50, 11.08 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15290/59736 [1:08:30<1:05:15, 11.35 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15300/59736 [1:08:33<1:46:00,  6.99 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15320/59736 [1:08:33<1:07:01, 11.05 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15330/59736 [1:08:34<1:08:03, 10.87 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15340/59736 [1:08:38<2:09:24,  5.72 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15350/59736 [1:08:40<1:57:34,  6.29 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15360/59736 [1:08:42<2:11:13,  5.64 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15370/59736 [1:08:48<3:43:27,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15380/59736 [1:08:52<4:11:37,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15390/59736 [1:08:53<3:05:20,  3.99 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15400/59736 [1:08:54<2:33:48,  4.80 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15410/59736 [1:08:54<1:58:01,  6.26 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15420/59736 [1:09:02<4:05:31,  3.01 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15430/59736 [1:09:04<3:36:59,  3.40 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15440/59736 [1:09:04<2:44:53,  4.48 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15450/59736 [1:09:06<2:38:01,  4.67 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15460/59736 [1:09:07<2:08:10,  5.76 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15470/59736 [1:09:09<2:19:54,  5.27 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15480/59736 [1:09:11<2:18:30,  5.33 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15490/59736 [1:09:12<2:03:14,  5.98 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15500/59736 [1:09:13<1:46:47,  6.90 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15510/59736 [1:09:14<1:26:59,  8.47 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15520/59736 [1:09:14<1:08:46, 10.72 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15530/59736 [1:09:15<1:18:24,  9.40 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15540/59736 [1:09:16<1:04:03, 11.50 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15550/59736 [1:09:16<55:16, 13.32 examples/s]  
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15560/59736 [1:09:17<1:05:29, 11.24 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15570/59736 [1:09:18<48:47, 15.09 examples/s]  
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15580/59736 [1:09:18<39:04, 18.83 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15590/59736 [1:09:18<34:25, 21.38 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15600/59736 [1:09:20<1:10:13, 10.48 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15610/59736 [1:09:25<2:34:28,  4.76 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15620/59736 [1:09:27<2:31:54,  4.84 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15630/59736 [1:09:27<1:51:12,  6.61 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15640/59736 [1:09:31<2:48:16,  4.37 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15640/59736 [1:09:44<2:48:16,  4.37 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15650/59736 [1:09:49<8:26:33,  1.45 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15660/59736 [1:09:50<6:07:35,  2.00 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15670/59736 [1:09:57<7:09:39,  1.71 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▌       | 15680/59736 [1:10:12<10:32:07,  1.16 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15690/59736 [1:10:13<7:38:19,  1.60 examples/s] 
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15700/59736 [1:10:20<7:49:54,  1.56 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15710/59736 [1:10:21<5:47:06,  2.11 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15720/59736 [1:10:21<4:18:01,  2.84 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15730/59736 [1:10:22<3:03:30,  4.00 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15740/59736 [1:10:25<3:29:40,  3.50 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15750/59736 [1:10:27<3:12:16,  3.81 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15760/59736 [1:10:29<2:44:00,  4.47 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15770/59736 [1:10:30<2:19:53,  5.24 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15780/59736 [1:10:30<1:40:31,  7.29 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15790/59736 [1:10:30<1:15:42,  9.67 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15800/59736 [1:10:34<2:07:36,  5.74 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15810/59736 [1:10:42<4:30:57,  2.70 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15820/59736 [1:10:44<3:50:28,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):  26%|██▋       | 15830/59736 [1:10:44<2:55:17,  4.17 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15840/59736 [1:10:48<3:20:54,  3.64 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15850/59736 [1:10:49<2:34:50,  4.72 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15860/59736 [1:10:51<2:30:39,  4.85 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15870/59736 [1:10:51<1:50:05,  6.64 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15880/59736 [1:10:54<2:21:18,  5.17 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15890/59736 [1:10:54<1:52:27,  6.50 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15900/59736 [1:10:56<2:05:54,  5.80 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15910/59736 [1:11:05<4:37:43,  2.63 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15920/59736 [1:11:06<3:31:38,  3.45 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15930/59736 [1:11:08<3:10:16,  3.84 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15940/59736 [1:11:09<2:34:39,  4.72 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15950/59736 [1:11:14<3:38:09,  3.35 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15960/59736 [1:11:19<4:24:35,  2.76 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15970/59736 [1:11:19<3:09:54,  3.84 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15980/59736 [1:11:19<2:19:14,  5.24 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 15990/59736 [1:11:27<4:22:56,  2.77 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16000/59736 [1:11:30<4:20:44,  2.80 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16010/59736 [1:11:32<3:29:19,  3.48 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16030/59736 [1:11:33<2:10:47,  5.57 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16040/59736 [1:11:34<2:01:00,  6.02 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16050/59736 [1:11:36<2:10:39,  5.57 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16060/59736 [1:11:39<2:27:52,  4.92 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16070/59736 [1:11:41<2:25:25,  5.00 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16080/59736 [1:11:41<1:57:48,  6.18 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16090/59736 [1:11:46<2:53:57,  4.18 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16100/59736 [1:11:47<2:21:17,  5.15 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16110/59736 [1:11:49<2:20:32,  5.17 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16120/59736 [1:11:50<2:17:43,  5.28 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16130/59736 [1:11:52<2:15:08,  5.38 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16140/59736 [1:11:54<2:22:44,  5.09 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16150/59736 [1:12:00<3:35:26,  3.37 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16160/59736 [1:12:03<3:40:21,  3.30 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16170/59736 [1:12:03<2:39:48,  4.54 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16180/59736 [1:12:04<2:15:20,  5.36 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16190/59736 [1:12:10<3:42:23,  3.26 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16200/59736 [1:12:11<2:51:14,  4.24 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16210/59736 [1:12:11<2:03:26,  5.88 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16220/59736 [1:12:13<2:23:04,  5.07 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16230/59736 [1:12:15<2:06:06,  5.75 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16240/59736 [1:12:28<6:11:06,  1.95 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16250/59736 [1:12:33<6:19:34,  1.91 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16260/59736 [1:12:34<4:49:18,  2.50 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16270/59736 [1:12:39<4:55:21,  2.45 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16280/59736 [1:12:43<4:58:48,  2.42 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16290/59736 [1:12:44<3:50:18,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16300/59736 [1:12:44<2:51:09,  4.23 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16310/59736 [1:12:48<3:12:33,  3.76 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16320/59736 [1:12:48<2:23:26,  5.04 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16330/59736 [1:12:54<3:48:25,  3.17 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16340/59736 [1:12:58<4:03:21,  2.97 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16350/59736 [1:13:00<3:47:34,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16360/59736 [1:13:01<2:48:32,  4.29 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16370/59736 [1:13:02<2:29:27,  4.84 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16380/59736 [1:13:08<3:54:57,  3.08 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16390/59736 [1:13:13<4:32:17,  2.65 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16410/59736 [1:13:14<2:44:21,  4.39 examples/s]
Running tokenizer on dataset (num_proc=64):  27%|██▋       | 16420/59736 [1:13:15<2:12:38,  5.44 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16430/59736 [1:13:16<2:03:39,  5.84 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16440/59736 [1:13:20<2:53:10,  4.17 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16450/59736 [1:13:21<2:07:14,  5.67 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16460/59736 [1:13:24<2:51:48,  4.20 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16470/59736 [1:13:26<2:37:15,  4.59 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16480/59736 [1:13:28<2:20:25,  5.13 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16490/59736 [1:13:28<1:42:35,  7.03 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16500/59736 [1:13:34<3:29:19,  3.44 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16510/59736 [1:13:35<2:43:24,  4.41 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16520/59736 [1:13:35<1:58:55,  6.06 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16530/59736 [1:13:37<2:09:55,  5.54 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16540/59736 [1:13:38<1:55:10,  6.25 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16550/59736 [1:13:41<2:07:34,  5.64 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16560/59736 [1:13:45<3:09:58,  3.79 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16570/59736 [1:13:53<4:58:40,  2.41 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16580/59736 [1:14:03<7:04:55,  1.69 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16590/59736 [1:14:13<8:23:57,  1.43 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16600/59736 [1:14:14<6:16:37,  1.91 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16610/59736 [1:14:14<4:26:09,  2.70 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16620/59736 [1:14:18<4:29:08,  2.67 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16640/59736 [1:14:19<2:40:53,  4.46 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16650/59736 [1:14:24<3:30:50,  3.41 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16660/59736 [1:14:25<2:53:43,  4.13 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16670/59736 [1:14:29<3:28:34,  3.44 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16690/59736 [1:14:31<2:31:52,  4.72 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16700/59736 [1:14:33<2:23:34,  5.00 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16710/59736 [1:14:35<2:21:56,  5.05 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16720/59736 [1:14:36<2:06:34,  5.66 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16740/59736 [1:14:37<1:29:25,  8.01 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16750/59736 [1:14:37<1:15:30,  9.49 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16760/59736 [1:14:42<2:25:52,  4.91 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16770/59736 [1:14:43<1:58:23,  6.05 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16780/59736 [1:14:45<2:03:59,  5.77 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16790/59736 [1:14:47<2:21:11,  5.07 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16800/59736 [1:14:48<2:00:56,  5.92 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16810/59736 [1:14:52<2:39:40,  4.48 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16820/59736 [1:14:58<4:01:11,  2.97 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16830/59736 [1:15:03<4:40:51,  2.55 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16840/59736 [1:15:04<3:37:06,  3.29 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16850/59736 [1:15:04<2:36:09,  4.58 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16860/59736 [1:15:05<2:09:31,  5.52 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16870/59736 [1:15:10<3:10:04,  3.76 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16880/59736 [1:15:14<3:46:33,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16890/59736 [1:15:16<3:25:15,  3.48 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16900/59736 [1:15:19<3:23:35,  3.51 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16910/59736 [1:15:21<3:00:40,  3.95 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16920/59736 [1:15:23<2:45:01,  4.32 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16940/59736 [1:15:32<3:58:17,  2.99 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16950/59736 [1:15:34<3:27:12,  3.44 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16960/59736 [1:15:35<3:06:48,  3.82 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16970/59736 [1:15:39<3:23:50,  3.50 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16980/59736 [1:15:43<3:45:45,  3.16 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 16990/59736 [1:15:46<3:37:38,  3.27 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 17000/59736 [1:15:47<3:07:59,  3.79 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 17010/59736 [1:15:47<2:18:37,  5.14 examples/s]
Running tokenizer on dataset (num_proc=64):  28%|██▊       | 17020/59736 [1:15:49<2:17:58,  5.16 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17030/59736 [1:15:52<2:23:46,  4.95 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17040/59736 [1:15:54<2:39:50,  4.45 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17050/59736 [1:15:55<2:06:15,  5.63 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17060/59736 [1:15:58<2:29:57,  4.74 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17070/59736 [1:16:03<3:36:11,  3.29 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17080/59736 [1:16:05<3:19:54,  3.56 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17090/59736 [1:16:06<2:22:31,  4.99 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17100/59736 [1:16:08<2:28:13,  4.79 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17110/59736 [1:16:08<1:46:41,  6.66 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17120/59736 [1:16:11<2:13:22,  5.33 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17130/59736 [1:16:11<1:47:19,  6.62 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17140/59736 [1:16:15<2:24:26,  4.92 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17150/59736 [1:16:16<2:05:54,  5.64 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17160/59736 [1:16:17<2:04:00,  5.72 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▊       | 17170/59736 [1:16:19<1:49:59,  6.45 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17180/59736 [1:16:22<2:27:35,  4.81 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17190/59736 [1:16:23<2:05:51,  5.63 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17200/59736 [1:16:26<2:31:13,  4.69 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17210/59736 [1:16:28<2:20:53,  5.03 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17220/59736 [1:16:33<3:33:15,  3.32 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17230/59736 [1:16:34<2:42:19,  4.36 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17240/59736 [1:16:38<3:18:10,  3.57 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17250/59736 [1:16:42<3:51:32,  3.06 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17260/59736 [1:16:42<2:53:45,  4.07 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17270/59736 [1:16:49<4:14:28,  2.78 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17280/59736 [1:16:49<3:08:33,  3.75 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17290/59736 [1:16:52<3:18:16,  3.57 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17300/59736 [1:16:53<2:39:39,  4.43 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17310/59736 [1:16:56<2:50:20,  4.15 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17320/59736 [1:17:01<3:42:42,  3.17 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17330/59736 [1:17:03<3:16:25,  3.60 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17340/59736 [1:17:07<3:35:41,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17350/59736 [1:17:07<2:33:16,  4.61 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17360/59736 [1:17:09<2:42:59,  4.33 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17370/59736 [1:17:10<2:02:34,  5.76 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17380/59736 [1:17:12<2:20:02,  5.04 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17400/59736 [1:17:16<2:13:49,  5.27 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17410/59736 [1:17:16<1:48:57,  6.47 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17420/59736 [1:17:28<4:59:49,  2.35 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17430/59736 [1:17:29<3:54:53,  3.00 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17440/59736 [1:17:34<4:18:57,  2.72 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17450/59736 [1:17:37<4:06:10,  2.86 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17460/59736 [1:17:42<4:34:37,  2.57 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17470/59736 [1:17:43<3:44:17,  3.14 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17480/59736 [1:17:45<3:24:01,  3.45 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17490/59736 [1:17:50<4:06:11,  2.86 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17500/59736 [1:17:52<3:37:26,  3.24 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17510/59736 [1:17:53<2:50:35,  4.13 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17520/59736 [1:17:54<2:08:13,  5.49 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17530/59736 [1:17:55<1:52:54,  6.23 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17540/59736 [1:17:55<1:35:36,  7.36 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17550/59736 [1:17:59<2:16:20,  5.16 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17560/59736 [1:17:59<1:37:54,  7.18 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17570/59736 [1:18:03<2:24:56,  4.85 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17580/59736 [1:18:06<2:47:51,  4.19 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17590/59736 [1:18:07<2:19:18,  5.04 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17600/59736 [1:18:07<1:47:44,  6.52 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17610/59736 [1:18:07<1:18:54,  8.90 examples/s]
Running tokenizer on dataset (num_proc=64):  29%|██▉       | 17620/59736 [1:18:15<3:30:41,  3.33 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17630/59736 [1:18:18<3:25:53,  3.41 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17640/59736 [1:18:21<3:43:04,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17650/59736 [1:18:23<3:14:01,  3.62 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17660/59736 [1:18:25<2:56:55,  3.96 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17670/59736 [1:18:29<3:18:59,  3.52 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17680/59736 [1:18:35<4:40:35,  2.50 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17690/59736 [1:18:38<4:07:21,  2.83 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17700/59736 [1:18:43<4:42:12,  2.48 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17710/59736 [1:18:43<3:26:59,  3.38 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17720/59736 [1:18:45<3:01:15,  3.86 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17730/59736 [1:18:46<2:16:07,  5.14 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17740/59736 [1:18:51<3:37:20,  3.22 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17750/59736 [1:18:52<2:41:25,  4.34 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17760/59736 [1:18:52<2:02:04,  5.73 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17770/59736 [1:18:57<3:13:52,  3.61 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17780/59736 [1:19:01<3:35:19,  3.25 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17790/59736 [1:19:01<2:35:34,  4.49 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17800/59736 [1:19:06<3:14:38,  3.59 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17810/59736 [1:19:07<2:43:37,  4.27 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17820/59736 [1:19:09<2:31:28,  4.61 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17830/59736 [1:19:12<3:06:00,  3.75 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17840/59736 [1:19:14<2:43:16,  4.28 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17850/59736 [1:19:15<2:07:16,  5.48 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17860/59736 [1:19:16<2:07:28,  5.47 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17870/59736 [1:19:21<3:01:08,  3.85 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17880/59736 [1:19:22<2:31:43,  4.60 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17890/59736 [1:19:25<2:56:28,  3.95 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17900/59736 [1:19:27<2:45:50,  4.20 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17910/59736 [1:19:34<4:05:47,  2.84 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|██▉       | 17920/59736 [1:19:34<2:59:31,  3.88 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 17930/59736 [1:19:37<3:16:42,  3.54 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 17940/59736 [1:19:38<2:36:53,  4.44 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 17950/59736 [1:19:42<3:03:48,  3.79 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 17960/59736 [1:19:48<4:17:33,  2.70 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 17970/59736 [1:19:51<4:08:35,  2.80 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 17980/59736 [1:19:55<4:20:34,  2.67 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 17990/59736 [1:19:56<3:23:20,  3.42 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18000/59736 [1:19:57<2:37:47,  4.41 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18010/59736 [1:19:58<2:13:38,  5.20 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18020/59736 [1:20:01<2:31:53,  4.58 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18030/59736 [1:20:03<2:15:53,  5.12 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18040/59736 [1:20:05<2:16:24,  5.09 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18050/59736 [1:20:05<1:46:05,  6.55 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18060/59736 [1:20:05<1:18:25,  8.86 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18070/59736 [1:20:10<2:35:11,  4.47 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18080/59736 [1:20:12<2:21:26,  4.91 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18090/59736 [1:20:12<1:45:07,  6.60 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18100/59736 [1:20:12<1:17:58,  8.90 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18110/59736 [1:20:18<2:50:18,  4.07 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18120/59736 [1:20:29<5:46:16,  2.00 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18130/59736 [1:20:31<4:48:19,  2.41 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18140/59736 [1:20:35<4:50:09,  2.39 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18150/59736 [1:20:43<6:03:01,  1.91 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18160/59736 [1:20:45<4:54:32,  2.35 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18170/59736 [1:20:47<4:16:36,  2.70 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18180/59736 [1:20:49<3:46:42,  3.05 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18190/59736 [1:20:50<2:53:54,  3.98 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18200/59736 [1:20:52<2:34:12,  4.49 examples/s]
Running tokenizer on dataset (num_proc=64):  30%|███       | 18210/59736 [1:20:54<2:37:24,  4.40 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18220/59736 [1:20:56<2:30:04,  4.61 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18230/59736 [1:20:56<1:50:54,  6.24 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18240/59736 [1:20:59<2:07:09,  5.44 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18250/59736 [1:21:03<3:07:01,  3.70 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18260/59736 [1:21:05<2:50:10,  4.06 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18270/59736 [1:21:06<2:18:52,  4.98 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18280/59736 [1:21:10<3:01:25,  3.81 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18290/59736 [1:21:19<5:00:15,  2.30 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18300/59736 [1:21:20<4:05:04,  2.82 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18310/59736 [1:21:21<2:57:29,  3.89 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18320/59736 [1:21:25<3:28:52,  3.30 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18330/59736 [1:21:25<2:28:48,  4.64 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18340/59736 [1:21:25<1:50:34,  6.24 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18350/59736 [1:21:25<1:19:55,  8.63 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18360/59736 [1:21:25<58:31, 11.78 examples/s]  
Running tokenizer on dataset (num_proc=64):  31%|███       | 18370/59736 [1:21:26<52:23, 13.16 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18380/59736 [1:21:30<1:53:13,  6.09 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18390/59736 [1:21:30<1:28:31,  7.78 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18400/59736 [1:21:31<1:17:43,  8.86 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18410/59736 [1:21:33<1:47:34,  6.40 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18410/59736 [1:21:44<1:47:34,  6.40 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18420/59736 [1:21:52<7:44:50,  1.48 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18430/59736 [1:21:56<6:41:08,  1.72 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18440/59736 [1:22:00<6:08:40,  1.87 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18450/59736 [1:22:03<5:14:48,  2.19 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18460/59736 [1:22:09<5:54:16,  1.94 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18470/59736 [1:22:14<5:34:32,  2.06 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18480/59736 [1:22:16<4:43:18,  2.43 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18490/59736 [1:22:29<7:52:58,  1.45 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18500/59736 [1:22:31<5:58:08,  1.92 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18510/59736 [1:22:36<6:08:32,  1.86 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18520/59736 [1:22:37<4:24:31,  2.60 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18530/59736 [1:22:38<3:21:00,  3.42 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18540/59736 [1:22:38<2:37:31,  4.36 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18550/59736 [1:22:44<3:37:21,  3.16 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18560/59736 [1:22:44<2:39:32,  4.30 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18570/59736 [1:22:45<2:21:57,  4.83 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18580/59736 [1:22:46<1:44:50,  6.54 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18590/59736 [1:22:46<1:30:15,  7.60 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18600/59736 [1:22:48<1:25:59,  7.97 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18610/59736 [1:22:48<1:07:54, 10.09 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18620/59736 [1:22:48<50:08, 13.66 examples/s]  
Running tokenizer on dataset (num_proc=64):  31%|███       | 18640/59736 [1:22:50<59:01, 11.60 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18650/59736 [1:22:52<1:20:25,  8.52 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███       | 18660/59736 [1:22:59<2:57:44,  3.85 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18670/59736 [1:23:01<2:50:18,  4.02 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18680/59736 [1:23:02<2:16:42,  5.01 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18690/59736 [1:23:03<2:07:00,  5.39 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18700/59736 [1:23:14<4:58:52,  2.29 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18710/59736 [1:23:20<5:29:04,  2.08 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18720/59736 [1:23:21<4:22:56,  2.60 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18730/59736 [1:23:22<3:23:28,  3.36 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18740/59736 [1:23:23<2:37:48,  4.33 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18750/59736 [1:23:26<2:49:35,  4.03 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18760/59736 [1:23:29<3:05:41,  3.68 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18770/59736 [1:23:29<2:17:50,  4.95 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18780/59736 [1:23:30<1:51:07,  6.14 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18800/59736 [1:23:33<1:40:05,  6.82 examples/s]
Running tokenizer on dataset (num_proc=64):  31%|███▏      | 18810/59736 [1:23:39<3:04:12,  3.70 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18820/59736 [1:23:41<2:48:34,  4.05 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18830/59736 [1:23:41<2:05:47,  5.42 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18840/59736 [1:23:46<2:59:06,  3.81 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18850/59736 [1:23:46<2:21:11,  4.83 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18860/59736 [1:23:49<2:28:22,  4.59 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18870/59736 [1:23:49<1:55:22,  5.90 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18880/59736 [1:23:51<1:57:23,  5.80 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18890/59736 [1:23:54<2:10:27,  5.22 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18900/59736 [1:23:54<1:42:00,  6.67 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18910/59736 [1:23:55<1:27:18,  7.79 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18920/59736 [1:23:59<2:29:20,  4.56 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18930/59736 [1:24:01<2:20:58,  4.82 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18940/59736 [1:24:01<1:45:30,  6.44 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18950/59736 [1:24:05<2:27:36,  4.61 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18960/59736 [1:24:08<2:47:35,  4.06 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18970/59736 [1:24:10<2:43:39,  4.15 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18980/59736 [1:24:18<4:26:45,  2.55 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 18990/59736 [1:24:19<3:26:57,  3.28 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19000/59736 [1:24:21<3:09:37,  3.58 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19010/59736 [1:24:27<4:17:05,  2.64 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19020/59736 [1:24:31<4:10:48,  2.71 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19030/59736 [1:24:31<3:00:07,  3.77 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19040/59736 [1:24:35<3:25:08,  3.31 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19050/59736 [1:24:41<4:35:34,  2.46 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19060/59736 [1:24:46<4:42:09,  2.40 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19070/59736 [1:24:47<3:52:58,  2.91 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19080/59736 [1:24:52<4:22:47,  2.58 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19100/59736 [1:24:55<3:08:34,  3.59 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19110/59736 [1:25:01<4:04:44,  2.77 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19130/59736 [1:25:04<2:52:50,  3.92 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19140/59736 [1:25:08<3:21:48,  3.35 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19150/59736 [1:25:08<2:36:42,  4.32 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19160/59736 [1:25:15<3:53:45,  2.89 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19170/59736 [1:25:18<3:51:04,  2.93 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19180/59736 [1:25:19<2:57:33,  3.81 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19190/59736 [1:25:20<2:23:03,  4.72 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19200/59736 [1:25:23<2:47:23,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19210/59736 [1:25:24<2:25:43,  4.63 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19220/59736 [1:25:26<2:21:17,  4.78 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19230/59736 [1:25:30<2:47:52,  4.02 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19240/59736 [1:25:35<3:43:17,  3.02 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19250/59736 [1:25:42<5:02:40,  2.23 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19260/59736 [1:25:45<4:23:57,  2.56 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19270/59736 [1:25:45<3:10:04,  3.55 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19280/59736 [1:25:47<2:52:40,  3.90 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19290/59736 [1:25:56<4:51:18,  2.31 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19300/59736 [1:25:56<3:28:41,  3.23 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19310/59736 [1:25:56<2:34:12,  4.37 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19320/59736 [1:25:59<2:53:39,  3.88 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19330/59736 [1:26:02<2:58:33,  3.77 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19350/59736 [1:26:04<1:57:41,  5.72 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19360/59736 [1:26:07<2:17:57,  4.88 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19370/59736 [1:26:18<5:08:47,  2.18 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19380/59736 [1:26:20<4:07:43,  2.72 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19390/59736 [1:26:23<4:11:44,  2.67 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19400/59736 [1:26:25<3:19:53,  3.36 examples/s]
Running tokenizer on dataset (num_proc=64):  32%|███▏      | 19410/59736 [1:26:25<2:28:22,  4.53 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19420/59736 [1:26:26<1:58:20,  5.68 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19430/59736 [1:26:30<2:54:24,  3.85 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19440/59736 [1:26:32<2:46:20,  4.04 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19450/59736 [1:26:34<2:32:34,  4.40 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19470/59736 [1:26:35<1:37:55,  6.85 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19480/59736 [1:26:37<1:41:58,  6.58 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19490/59736 [1:26:38<1:40:13,  6.69 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19500/59736 [1:26:41<2:03:52,  5.41 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19510/59736 [1:26:42<1:43:00,  6.51 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19520/59736 [1:26:45<2:08:52,  5.20 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19530/59736 [1:26:50<3:17:11,  3.40 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19540/59736 [1:26:51<2:33:36,  4.36 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19550/59736 [1:26:54<2:53:33,  3.86 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19560/59736 [1:26:55<2:24:18,  4.64 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19570/59736 [1:26:58<2:27:53,  4.53 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19580/59736 [1:26:58<1:46:50,  6.26 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19590/59736 [1:27:00<2:04:09,  5.39 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19600/59736 [1:27:06<3:29:27,  3.19 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19610/59736 [1:27:07<2:38:10,  4.23 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19620/59736 [1:27:11<3:11:48,  3.49 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19630/59736 [1:27:14<3:17:05,  3.39 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19640/59736 [1:27:17<3:19:25,  3.35 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19650/59736 [1:27:19<2:58:34,  3.74 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19660/59736 [1:27:22<2:55:51,  3.80 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19670/59736 [1:27:32<5:39:57,  1.96 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19680/59736 [1:27:36<5:06:30,  2.18 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19690/59736 [1:27:38<4:17:42,  2.59 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19700/59736 [1:27:38<3:06:25,  3.58 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19710/59736 [1:27:46<4:41:27,  2.37 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19720/59736 [1:27:47<3:31:51,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19730/59736 [1:27:47<2:42:47,  4.10 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19750/59736 [1:27:48<1:42:52,  6.48 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19770/59736 [1:27:50<1:18:17,  8.51 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19780/59736 [1:27:50<1:04:24, 10.34 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19790/59736 [1:27:54<1:50:52,  6.00 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19800/59736 [1:27:55<1:50:38,  6.02 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19810/59736 [1:28:06<4:26:55,  2.49 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19820/59736 [1:28:11<4:46:31,  2.32 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19830/59736 [1:28:11<3:36:09,  3.08 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19840/59736 [1:28:12<2:45:35,  4.02 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19850/59736 [1:28:23<5:31:46,  2.00 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19860/59736 [1:28:24<4:04:32,  2.72 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19870/59736 [1:28:27<4:04:47,  2.71 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19880/59736 [1:28:42<7:34:06,  1.46 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19900/59736 [1:28:44<4:45:30,  2.33 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19910/59736 [1:28:45<3:41:34,  3.00 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19920/59736 [1:28:48<3:43:47,  2.97 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19940/59736 [1:28:49<2:14:46,  4.92 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19950/59736 [1:28:49<1:53:49,  5.83 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19960/59736 [1:28:52<2:15:07,  4.91 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19980/59736 [1:28:53<1:23:56,  7.89 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 19990/59736 [1:28:53<1:08:25,  9.68 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 20000/59736 [1:28:58<2:06:12,  5.25 examples/s]
Running tokenizer on dataset (num_proc=64):  33%|███▎      | 20010/59736 [1:29:01<2:34:07,  4.30 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20020/59736 [1:29:01<1:55:55,  5.71 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20030/59736 [1:29:05<2:30:43,  4.39 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20040/59736 [1:29:05<1:54:48,  5.76 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20050/59736 [1:29:08<2:17:01,  4.83 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20060/59736 [1:29:10<2:09:25,  5.11 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20070/59736 [1:29:15<3:11:15,  3.46 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20080/59736 [1:29:17<3:04:05,  3.59 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20090/59736 [1:29:18<2:23:23,  4.61 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20100/59736 [1:29:23<3:17:36,  3.34 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20110/59736 [1:29:23<2:25:41,  4.53 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20120/59736 [1:29:27<2:52:55,  3.82 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20130/59736 [1:29:29<2:30:02,  4.40 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20140/59736 [1:29:32<2:55:52,  3.75 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▎      | 20160/59736 [1:29:40<3:27:44,  3.18 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20170/59736 [1:29:41<2:59:38,  3.67 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20180/59736 [1:29:43<2:55:44,  3.75 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20190/59736 [1:29:44<2:11:13,  5.02 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20200/59736 [1:29:44<1:48:29,  6.07 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20210/59736 [1:29:46<1:48:52,  6.05 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20220/59736 [1:29:47<1:29:58,  7.32 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20230/59736 [1:29:49<1:38:24,  6.69 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20240/59736 [1:30:02<5:31:18,  1.99 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20250/59736 [1:30:04<4:21:55,  2.51 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20260/59736 [1:30:06<3:43:24,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20270/59736 [1:30:10<4:04:08,  2.69 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20280/59736 [1:30:11<3:11:09,  3.44 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20290/59736 [1:30:13<2:50:23,  3.86 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20300/59736 [1:30:13<2:08:09,  5.13 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20310/59736 [1:30:15<2:05:17,  5.24 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20320/59736 [1:30:19<2:37:44,  4.16 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20330/59736 [1:30:19<2:02:33,  5.36 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20340/59736 [1:30:20<1:37:23,  6.74 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20350/59736 [1:30:20<1:11:37,  9.16 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20360/59736 [1:30:21<1:13:36,  8.92 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20370/59736 [1:30:22<1:09:50,  9.40 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20380/59736 [1:30:28<2:38:33,  4.14 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20390/59736 [1:30:28<2:01:13,  5.41 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20400/59736 [1:30:30<1:56:58,  5.60 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20410/59736 [1:30:33<2:20:47,  4.66 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20420/59736 [1:30:38<3:27:16,  3.16 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20430/59736 [1:30:41<3:14:05,  3.38 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20440/59736 [1:30:48<4:36:02,  2.37 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20450/59736 [1:30:50<3:42:39,  2.94 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20460/59736 [1:30:52<3:27:37,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20470/59736 [1:30:53<2:31:13,  4.33 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20480/59736 [1:31:04<5:32:31,  1.97 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20490/59736 [1:31:08<5:16:12,  2.07 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20500/59736 [1:31:09<3:45:37,  2.90 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20510/59736 [1:31:14<4:22:11,  2.49 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20530/59736 [1:31:17<3:02:18,  3.58 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20540/59736 [1:31:17<2:24:33,  4.52 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20550/59736 [1:31:19<2:13:41,  4.89 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20560/59736 [1:31:21<2:24:44,  4.51 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20570/59736 [1:31:27<3:27:14,  3.15 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20580/59736 [1:31:29<3:04:40,  3.53 examples/s]
Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20590/59736 [1:31:30<2:22:34,  4.58 examples/s]W1205 10:25:27.556000 402168 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers
W1205 10:25:27.562000 402168 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 402763 closing signal SIGINT
W1205 10:25:27.563000 402168 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 402765 closing signal SIGINT
W1205 10:25:27.564000 402168 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 402766 closing signal SIGINT
W1205 10:25:27.565000 402168 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 402767 closing signal SIGINT
W1205 10:25:27.565000 402168 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 402768 closing signal SIGINT
W1205 10:25:27.566000 402168 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 402769 closing signal SIGINT
W1205 10:25:27.566000 402168 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 402844 closing signal SIGINT
W1205 10:25:27.567000 402168 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 402845 closing signal SIGINT
Process ForkPoolWorker-108:
Process ForkPoolWorker-121:
Process ForkPoolWorker-115:
Process ForkPoolWorker-82:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-118:
Process ForkPoolWorker-114:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-97:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-92:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
Process ForkPoolWorker-70:
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-80:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
KeyboardInterrupt
Process ForkPoolWorker-112:
Process ForkPoolWorker-76:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt

Running tokenizer on dataset (num_proc=64):  34%|███▍      | 20590/59736 [1:31:31<2:54:00,  3.75 examples/s]Traceback (most recent call last):

  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-91:
Process ForkPoolWorker-74:
Process ForkPoolWorker-83:
Process ForkPoolWorker-89:
Process ForkPoolWorker-84:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-98:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 733, in _get_mm_inputs
    mm_inputs.update(image_processor(**input_dict, return_tensors="pt"))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 41, in __call__
    return self.preprocess(images, **kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py", line 439, in preprocess
    patches, video_grid_thw = self._preprocess(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py", line 291, in _preprocess
    patches = np.array(processed_images)
KeyboardInterrupt
Process ForkPoolWorker-90:
Process ForkPoolWorker-102:
Process ForkPoolWorker-71:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-88:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-85:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/bin/llamafactory-cli", line 8, in <module>
    sys.exit(main())
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/cli.py", line 94, in main
    process = subprocess.run(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
KeyboardInterrupt
Exception ignored in atexit callback: <bound method TemporaryDirectory.cleanup of <TemporaryDirectory '/tmp/tmpekrhqg2kwandb-media'>>
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/tempfile.py", line 882, in cleanup
Process ForkPoolWorker-106:
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/shutil.py", line 713, in rmtree
Process ForkPoolWorker-113:
    orig_st = os.lstat(path)
KeyboardInterrupt: 
Process ForkPoolWorker-105:
Process ForkPoolWorker-126:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 198, in _regularize_videos
    #                 frames.append(frame.to_image())
  File "av/video/frame.pyx", line 291, in av.video.frame.VideoFrame.to_image
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/PIL/Image.py", line 3137, in frombytes
    im = new(mode, size)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/PIL/Image.py", line 3102, in new
    return im._new(core.fill(mode, size, color))
KeyboardInterrupt
Process ForkPoolWorker-77:
Process ForkPoolWorker-87:
Process ForkPoolWorker-79:
Process ForkPoolWorker-120:
Process ForkPoolWorker-101:
Process ForkPoolWorker-116:
Process ForkPoolWorker-124:
Process ForkPoolWorker-86:
Process ForkPoolWorker-122:
Process ForkPoolWorker-95:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-93:
Process ForkPoolWorker-109:
Process ForkPoolWorker-107:
Process ForkPoolWorker-94:
Process ForkPoolWorker-128:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-125:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-96:
Process ForkPoolWorker-103:
Process ForkPoolWorker-119:
Process ForkPoolWorker-123:
Process ForkPoolWorker-111:
Process ForkPoolWorker-100:
Process ForkPoolWorker-110:
Process ForkPoolWorker-129:
Process ForkPoolWorker-81:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Process ForkPoolWorker-117:
Process ForkPoolWorker-104:
Process ForkPoolWorker-99:
Process ForkPoolWorker-127:
Process ForkPoolWorker-78:
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 733, in _get_mm_inputs
    mm_inputs.update(image_processor(**input_dict, return_tensors="pt"))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 41, in __call__
    return self.preprocess(images, **kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py", line 439, in preprocess
    patches, video_grid_thw = self._preprocess(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py", line 284, in _preprocess
    image = self.normalize(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 111, in normalize
    return normalize(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/transformers/image_transforms.py", line 409, in normalize
    image = to_channel_dimension_format(image, data_format, input_data_format) if data_format is not None else image
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 198, in _regularize_videos
    #                 frames.append(frame.to_image())
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "av/video/frame.pyx", line 274, in av.video.frame.VideoFrame.to_image
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "<frozen importlib._bootstrap>", line 1053, in _handle_fromlist
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
KeyboardInterrupt
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x7f260c298f70>
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 473, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 450, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 173, in _update_autotune_table
    cache_manager = AutotuneCacheManager(cache_key)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 82, in __init__
    self.cache_dir = os.environ.get('TRITON_CACHE_DIR', TritonCacheDir.default_cache_dir())
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 45, in default_cache_dir
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
    if is_nfs_path(tmp_path) and not TritonCacheDir._warning_printed:
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 27, in is_nfs_path
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
    output = subprocess.check_output(['df', '-T', path], encoding='utf-8')
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 421, in check_output
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 503, in run
    with Popen(*popenargs, **kwargs) as process:
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 971, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/subprocess.py", line 1809, in _execute_child
    os.close(errpipe_write)
KeyboardInterrupt: 
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 614, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3470, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/mnt/petrelfs/weixilin/miniconda3/envs/qwen2-vl/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3349, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 105, in preprocess_supervised_dataset
    input_ids, labels = _encode_supervised_example(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/processors/supervised.py", line 48, in _encode_supervised_example
    messages = template.mm_plugin.process_messages(prompt + response, images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 748, in process_messages
    mm_inputs = self._get_mm_inputs(images, videos, processor)
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 715, in _get_mm_inputs
    videos = self._regularize_videos(
  File "/mnt/petrelfs/weixilin/projects/MLLM/LLaMA-Factory/src/llamafactory/data/mm_plugin.py", line 197, in _regularize_videos
    #             if frame_idx in sample_indices:
KeyboardInterrupt
